{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_webkb(data_name):\n",
    "    path = f'/storage/qiaoyr/TAPE/dataset/web_kb/WebKB/{data_name}'\n",
    "    webpage_features_labels = np.genfromtxt(\"{}.content\".format(path), dtype=np.dtype(str))\n",
    "    data_X = webpage_features_labels[:, 1:-1].astype(np.float32)\n",
    "    labels = webpage_features_labels[:, -1]\n",
    "    #print(labels)\n",
    "    class_map = {x: i for i, x in enumerate(['course', 'faculty', 'student','project', 'staff'])}  \n",
    "    #print(class_map)\n",
    "    data_Y = np.array([class_map[x] for x in labels])\n",
    "    data_webpage_url = webpage_features_labels[:, 0]\n",
    "    # data_webpage_id = np.arange(len(data_webpage_url))\n",
    "    data_webpage_id_map = {x: i for i, x in enumerate(data_webpage_url)}\n",
    "    edges_unordered = np.genfromtxt(\"{}.cites\".format(path), dtype=np.dtype(str))\n",
    "    '''\n",
    "    for i in range(edges_unordered.shape[0]):\n",
    "        if edges_unordered[i][0] == edges_unordered[i][1]:\n",
    "            print('self loop:',edges_unordered[i][0])\n",
    "    '''\n",
    "    edges = np.array(list(map(data_webpage_id_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    #print(edges.shape)\n",
    "    data_edges = np.array(edges[~(edges == None).max(1)], dtype=np.int32)\n",
    "    #print(data_edges.shape)\n",
    "    data_edges = np.vstack((data_edges, np.fliplr(data_edges)))\n",
    "    #print(data_edges.shape)\n",
    "\n",
    "    return data_X, data_Y, data_webpage_url, np.unique(data_edges, axis=0).transpose()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "'''\n",
    "X, Y, webpage_id, edges = parse_wisconsin()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(webpage_id.shape)\n",
    "print(edges.shape)\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nX, Y, webpage_id, edges = parse_wisconsin()\\nprint(X.shape)\\nprint(Y.shape)\\nprint(webpage_id.shape)\\nprint(edges.shape)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "def get_webkb_casestudy(data_name, SEED=0):\n",
    "    data_X, data_Y, data_webpage_url, data_edges = parse_webkb(data_name)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)  # Numpy module.\n",
    "    random.seed(SEED)  # Python random module.\n",
    "\n",
    "    # load data\n",
    "    data = Data(x=torch.tensor(data_X).float(),\n",
    "                 edge_index=torch.tensor(data_edges).long(), \n",
    "                 y=torch.tensor(data_Y).long(),\n",
    "                 num_nodes=len(data_Y))\n",
    "    # split data\n",
    "    node_id = np.arange(data.num_nodes)\n",
    "    np.random.shuffle(node_id)\n",
    "\n",
    "    data.train_id = np.sort(node_id[:int(data.num_nodes * 0.6)])\n",
    "    data.val_id = np.sort(\n",
    "        node_id[int(data.num_nodes * 0.6):int(data.num_nodes * 0.8)])\n",
    "    data.test_id = np.sort(node_id[int(data.num_nodes * 0.8):])\n",
    "\n",
    "    data.train_mask = torch.tensor(\n",
    "        [x in data.train_id for x in range(data.num_nodes)])\n",
    "    data.val_mask = torch.tensor(\n",
    "        [x in data.val_id for x in range(data.num_nodes)])\n",
    "    data.test_mask = torch.tensor(\n",
    "        [x in data.test_id for x in range(data.num_nodes)])\n",
    "    \n",
    "    return data, data_webpage_url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "def html_process(input_string):\n",
    "    # 使用正则表达式去掉所有 HTML 标签\n",
    "    lines = input_string.split('\\n')\n",
    "    clean_text = ' '.join(lines[6:])\n",
    "\n",
    "    #non_empty_lines = [line for line in clean_text if line.strip()]\n",
    "    \n",
    "    #tag_list = ['<.*?>', r'<ahref\\s*=\\s*\".*?\"\\s*>', r'<a\\shref\\s*=\\s*\".*?\"\\s*>', r'<meta *.html>', r'<img src*>', r'<IMG SRC*\">', r'<bodyBACKGROUND*>', r'<imgsrc*>', r'<AHREF*>', '\\n']\n",
    "    tag_list = ['<.*?>', '\\n', r'<a\\s+href\\s*=\\s*\".*?\"\\s*>', r'<IMG\\s+SRC\\s*=\\s*\".*?\"\\s+ALT\\s*=\\s*\".*?\"\\s*>']\n",
    "    for tag in tag_list:\n",
    "        clean_text = re.sub(tag, '', clean_text, flags=re.IGNORECASE)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    return clean_text\n",
    "'''\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        try:\n",
    "            file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        except:\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            text.append('')\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text\n",
    "'''\n",
    "def delete_vacant_webpage(data, i):\n",
    "    data.y = torch.cat((data.y[:i], data.y[(i+1):]))\n",
    "    # data.edge_index = torch.cat((data.edge_index[:,:i], data.edge_index[:,(i+1):]), dim=1)\n",
    "    data.x = torch.cat((data.x[:i], data.x[(i+1):]))\n",
    "    data.train_mask = torch.cat((data.train_mask[:i], data.train_mask[(i+1):]))\n",
    "    data.val_mask = torch.cat((data.val_mask[:i], data.val_mask[(i+1):]))\n",
    "    data.test_mask = torch.cat((data.test_mask[:i], data.test_mask[(i+1):]))\n",
    "    data.train_id = np.delete(data.train_id, i)\n",
    "    data.val_id = np.delete(data.val_id, i)\n",
    "    data.test_id = np.delete(data.test_id, i)\n",
    "    data.num_nodes -= 1\n",
    "    mask = (data.edge_index[0] == i) | (data.edge_index[1] == i)\n",
    "    data.edge_index = data.edge_index[:,~mask] \n",
    "    return data\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        webs_to_remove = []\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "        if os.path.exists(file_path):\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        else:\n",
    "            print(i)\n",
    "            print(i)\n",
    "            webs_to_remove.append(i)\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            # text.append('')\n",
    "            \n",
    "    print(webs_to_remove)\n",
    "   \n",
    "    for i in reversed(webs_to_remove):\n",
    "        print(i)\n",
    "        data = delete_vacant_webpage(data, i)\n",
    "        print(data)\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "data, clean_text = get_raw_text_webkb('wisconsin', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3\n",
      "3\n",
      "3 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/wisconsin/http:^^www.cs.wisc.edu^ not found\n",
      "5\n",
      "5\n",
      "5 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/wisconsin/http:^^www.cs.wisc.edu^condor^next.html not found\n",
      "[]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_webkb(data_name):\n",
    "    path = f'/storage/qiaoyr/TAPE/dataset/web_kb/WebKB/{data_name}'\n",
    "    webpage_features_labels = np.genfromtxt(\"{}.content\".format(path), dtype=np.dtype(str))\n",
    "    data_X = webpage_features_labels[:, 1:-1].astype(np.float32)\n",
    "    labels = webpage_features_labels[:, -1]\n",
    "    #print(labels)\n",
    "    class_map = {x: i for i, x in enumerate(['course', 'faculty', 'student','project', 'staff'])}  \n",
    "    #print(class_map)\n",
    "    data_Y = np.array([class_map[x] for x in labels])\n",
    "    data_webpage_url = webpage_features_labels[:, 0]\n",
    "    # data_webpage_id = np.arange(len(data_webpage_url))\n",
    "    data_webpage_id_map = {x: i for i, x in enumerate(data_webpage_url)}\n",
    "    edges_unordered = np.genfromtxt(\"{}.cites\".format(path), dtype=np.dtype(str))\n",
    "    '''\n",
    "    for i in range(edges_unordered.shape[0]):\n",
    "        if edges_unordered[i][0] == edges_unordered[i][1]:\n",
    "            print('self loop:',edges_unordered[i][0])\n",
    "    '''\n",
    "    edges = np.array(list(map(data_webpage_id_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    #print(edges.shape)\n",
    "    data_edges = np.array(edges[~(edges == None).max(1)], dtype=np.int32)\n",
    "    #print(data_edges.shape)\n",
    "    data_edges = np.vstack((data_edges, np.fliplr(data_edges)))\n",
    "    #print(data_edges.shape)\n",
    "\n",
    "    return data_X, data_Y, data_webpage_url, np.unique(data_edges, axis=0).transpose()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "'''\n",
    "X, Y, webpage_id, edges = parse_wisconsin()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(webpage_id.shape)\n",
    "print(edges.shape)\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nX, Y, webpage_id, edges = parse_wisconsin()\\nprint(X.shape)\\nprint(Y.shape)\\nprint(webpage_id.shape)\\nprint(edges.shape)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "def get_webkb_casestudy(data_name, SEED=0):\n",
    "    data_X, data_Y, data_webpage_url, data_edges = parse_webkb(data_name)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)  # Numpy module.\n",
    "    random.seed(SEED)  # Python random module.\n",
    "\n",
    "    # load data\n",
    "    data = Data(x=torch.tensor(data_X).float(),\n",
    "                 edge_index=torch.tensor(data_edges).long(), \n",
    "                 y=torch.tensor(data_Y).long(),\n",
    "                 num_nodes=len(data_Y))\n",
    "    # split data\n",
    "    node_id = np.arange(data.num_nodes)\n",
    "    np.random.shuffle(node_id)\n",
    "\n",
    "    data.train_id = np.sort(node_id[:int(data.num_nodes * 0.6)])\n",
    "    data.val_id = np.sort(\n",
    "        node_id[int(data.num_nodes * 0.6):int(data.num_nodes * 0.8)])\n",
    "    data.test_id = np.sort(node_id[int(data.num_nodes * 0.8):])\n",
    "\n",
    "    data.train_mask = torch.tensor(\n",
    "        [x in data.train_id for x in range(data.num_nodes)])\n",
    "    data.val_mask = torch.tensor(\n",
    "        [x in data.val_id for x in range(data.num_nodes)])\n",
    "    data.test_mask = torch.tensor(\n",
    "        [x in data.test_id for x in range(data.num_nodes)])\n",
    "    \n",
    "    return data, data_webpage_url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "def html_process(input_string):\n",
    "    # 使用正则表达式去掉所有 HTML 标签\n",
    "    lines = input_string.split('\\n')\n",
    "    clean_text = ' '.join(lines[6:])\n",
    "\n",
    "    #non_empty_lines = [line for line in clean_text if line.strip()]\n",
    "    \n",
    "    #tag_list = ['<.*?>', r'<ahref\\s*=\\s*\".*?\"\\s*>', r'<a\\shref\\s*=\\s*\".*?\"\\s*>', r'<meta *.html>', r'<img src*>', r'<IMG SRC*\">', r'<bodyBACKGROUND*>', r'<imgsrc*>', r'<AHREF*>', '\\n']\n",
    "    tag_list = ['<.*?>', '\\n', r'<a\\s+href\\s*=\\s*\".*?\"\\s*>', r'<IMG\\s+SRC\\s*=\\s*\".*?\"\\s+ALT\\s*=\\s*\".*?\"\\s*>']\n",
    "    for tag in tag_list:\n",
    "        clean_text = re.sub(tag, '', clean_text, flags=re.IGNORECASE)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    return clean_text\n",
    "'''\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        try:\n",
    "            file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        except:\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            text.append('')\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text\n",
    "'''\n",
    "def delete_vacant_webpage(data, i):\n",
    "    data.y = torch.cat((data.y[:i], data.y[(i+1):]))\n",
    "    # data.edge_index = torch.cat((data.edge_index[:,:i], data.edge_index[:,(i+1):]), dim=1)\n",
    "    data.x = torch.cat((data.x[:i], data.x[(i+1):]))\n",
    "    data.train_mask = torch.cat((data.train_mask[:i], data.train_mask[(i+1):]))\n",
    "    data.val_mask = torch.cat((data.val_mask[:i], data.val_mask[(i+1):]))\n",
    "    data.test_mask = torch.cat((data.test_mask[:i], data.test_mask[(i+1):]))\n",
    "    data.train_id = np.delete(data.train_id, i)\n",
    "    data.val_id = np.delete(data.val_id, i)\n",
    "    data.test_id = np.delete(data.test_id, i)\n",
    "    data.num_nodes -= 1\n",
    "    mask = (data.edge_index[0] == i) | (data.edge_index[1] == i)\n",
    "    data.edge_index = data.edge_index[:,~mask] \n",
    "    return data\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        pages_to_remove = []\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "        if os.path.exists(file_path):\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        else:\n",
    "            pages_to_remove.append(i)\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            # text.append('')\n",
    "            \n",
    "    print(pages_to_remove)\n",
    "   \n",
    "    for i in reversed(pages_to_remove):\n",
    "        print(i)\n",
    "        data = delete_vacant_webpage(data, i)\n",
    "        print(data)\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "data, clean_text = get_raw_text_webkb('wisconsin', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/wisconsin/http:^^www.cs.wisc.edu^ not found\n",
      "5 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/wisconsin/http:^^www.cs.wisc.edu^condor^next.html not found\n",
      "[]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_webkb(data_name):\n",
    "    path = f'/storage/qiaoyr/TAPE/dataset/web_kb/WebKB/{data_name}'\n",
    "    webpage_features_labels = np.genfromtxt(\"{}.content\".format(path), dtype=np.dtype(str))\n",
    "    data_X = webpage_features_labels[:, 1:-1].astype(np.float32)\n",
    "    labels = webpage_features_labels[:, -1]\n",
    "    #print(labels)\n",
    "    class_map = {x: i for i, x in enumerate(['course', 'faculty', 'student','project', 'staff'])}  \n",
    "    #print(class_map)\n",
    "    data_Y = np.array([class_map[x] for x in labels])\n",
    "    data_webpage_url = webpage_features_labels[:, 0]\n",
    "    # data_webpage_id = np.arange(len(data_webpage_url))\n",
    "    data_webpage_id_map = {x: i for i, x in enumerate(data_webpage_url)}\n",
    "    edges_unordered = np.genfromtxt(\"{}.cites\".format(path), dtype=np.dtype(str))\n",
    "    '''\n",
    "    for i in range(edges_unordered.shape[0]):\n",
    "        if edges_unordered[i][0] == edges_unordered[i][1]:\n",
    "            print('self loop:',edges_unordered[i][0])\n",
    "    '''\n",
    "    edges = np.array(list(map(data_webpage_id_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    #print(edges.shape)\n",
    "    data_edges = np.array(edges[~(edges == None).max(1)], dtype=np.int32)\n",
    "    #print(data_edges.shape)\n",
    "    data_edges = np.vstack((data_edges, np.fliplr(data_edges)))\n",
    "    #print(data_edges.shape)\n",
    "\n",
    "    return data_X, data_Y, data_webpage_url, np.unique(data_edges, axis=0).transpose()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "'''\n",
    "X, Y, webpage_id, edges = parse_wisconsin()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(webpage_id.shape)\n",
    "print(edges.shape)\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nX, Y, webpage_id, edges = parse_wisconsin()\\nprint(X.shape)\\nprint(Y.shape)\\nprint(webpage_id.shape)\\nprint(edges.shape)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "def get_webkb_casestudy(data_name, SEED=0):\n",
    "    data_X, data_Y, data_webpage_url, data_edges = parse_webkb(data_name)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)  # Numpy module.\n",
    "    random.seed(SEED)  # Python random module.\n",
    "\n",
    "    # load data\n",
    "    data = Data(x=torch.tensor(data_X).float(),\n",
    "                 edge_index=torch.tensor(data_edges).long(), \n",
    "                 y=torch.tensor(data_Y).long(),\n",
    "                 num_nodes=len(data_Y))\n",
    "    # split data\n",
    "    node_id = np.arange(data.num_nodes)\n",
    "    np.random.shuffle(node_id)\n",
    "\n",
    "    data.train_id = np.sort(node_id[:int(data.num_nodes * 0.6)])\n",
    "    data.val_id = np.sort(\n",
    "        node_id[int(data.num_nodes * 0.6):int(data.num_nodes * 0.8)])\n",
    "    data.test_id = np.sort(node_id[int(data.num_nodes * 0.8):])\n",
    "\n",
    "    data.train_mask = torch.tensor(\n",
    "        [x in data.train_id for x in range(data.num_nodes)])\n",
    "    data.val_mask = torch.tensor(\n",
    "        [x in data.val_id for x in range(data.num_nodes)])\n",
    "    data.test_mask = torch.tensor(\n",
    "        [x in data.test_id for x in range(data.num_nodes)])\n",
    "    \n",
    "    return data, data_webpage_url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "def html_process(input_string):\n",
    "    # 使用正则表达式去掉所有 HTML 标签\n",
    "    lines = input_string.split('\\n')\n",
    "    clean_text = ' '.join(lines[6:])\n",
    "\n",
    "    #non_empty_lines = [line for line in clean_text if line.strip()]\n",
    "    \n",
    "    #tag_list = ['<.*?>', r'<ahref\\s*=\\s*\".*?\"\\s*>', r'<a\\shref\\s*=\\s*\".*?\"\\s*>', r'<meta *.html>', r'<img src*>', r'<IMG SRC*\">', r'<bodyBACKGROUND*>', r'<imgsrc*>', r'<AHREF*>', '\\n']\n",
    "    tag_list = ['<.*?>', '\\n', r'<a\\s+href\\s*=\\s*\".*?\"\\s*>', r'<IMG\\s+SRC\\s*=\\s*\".*?\"\\s+ALT\\s*=\\s*\".*?\"\\s*>']\n",
    "    for tag in tag_list:\n",
    "        clean_text = re.sub(tag, '', clean_text, flags=re.IGNORECASE)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    return clean_text\n",
    "'''\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        try:\n",
    "            file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        except:\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            text.append('')\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text\n",
    "'''\n",
    "def delete_vacant_webpage(data, i):\n",
    "    data.y = torch.cat((data.y[:i], data.y[(i+1):]))\n",
    "    # data.edge_index = torch.cat((data.edge_index[:,:i], data.edge_index[:,(i+1):]), dim=1)\n",
    "    data.x = torch.cat((data.x[:i], data.x[(i+1):]))\n",
    "    data.train_mask = torch.cat((data.train_mask[:i], data.train_mask[(i+1):]))\n",
    "    data.val_mask = torch.cat((data.val_mask[:i], data.val_mask[(i+1):]))\n",
    "    data.test_mask = torch.cat((data.test_mask[:i], data.test_mask[(i+1):]))\n",
    "    data.train_id = np.delete(data.train_id, i)\n",
    "    data.val_id = np.delete(data.val_id, i)\n",
    "    data.test_id = np.delete(data.test_id, i)\n",
    "    data.num_nodes -= 1\n",
    "    mask = (data.edge_index[0] == i) | (data.edge_index[1] == i)\n",
    "    data.edge_index = data.edge_index[:,~mask] \n",
    "    return data\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        pages_to_remove = []\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "        if os.path.exists(file_path):\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        else:\n",
    "            pages_to_remove.append(i)\n",
    "            # print(i, file_path, 'not found') ###TODO\n",
    "            # text.append('')\n",
    "            \n",
    "    print(pages_to_remove)\n",
    "   \n",
    "    for i in reversed(pages_to_remove):\n",
    "        print(i)\n",
    "        data = delete_vacant_webpage(data, i)\n",
    "        print(data)\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "data, clean_text = get_raw_text_webkb('wisconsin', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_webkb(data_name):\n",
    "    path = f'/storage/qiaoyr/TAPE/dataset/web_kb/WebKB/{data_name}'\n",
    "    webpage_features_labels = np.genfromtxt(\"{}.content\".format(path), dtype=np.dtype(str))\n",
    "    data_X = webpage_features_labels[:, 1:-1].astype(np.float32)\n",
    "    labels = webpage_features_labels[:, -1]\n",
    "    #print(labels)\n",
    "    class_map = {x: i for i, x in enumerate(['course', 'faculty', 'student','project', 'staff'])}  \n",
    "    #print(class_map)\n",
    "    data_Y = np.array([class_map[x] for x in labels])\n",
    "    data_webpage_url = webpage_features_labels[:, 0]\n",
    "    # data_webpage_id = np.arange(len(data_webpage_url))\n",
    "    data_webpage_id_map = {x: i for i, x in enumerate(data_webpage_url)}\n",
    "    edges_unordered = np.genfromtxt(\"{}.cites\".format(path), dtype=np.dtype(str))\n",
    "    '''\n",
    "    for i in range(edges_unordered.shape[0]):\n",
    "        if edges_unordered[i][0] == edges_unordered[i][1]:\n",
    "            print('self loop:',edges_unordered[i][0])\n",
    "    '''\n",
    "    edges = np.array(list(map(data_webpage_id_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    #print(edges.shape)\n",
    "    data_edges = np.array(edges[~(edges == None).max(1)], dtype=np.int32)\n",
    "    #print(data_edges.shape)\n",
    "    data_edges = np.vstack((data_edges, np.fliplr(data_edges)))\n",
    "    #print(data_edges.shape)\n",
    "\n",
    "    return data_X, data_Y, data_webpage_url, np.unique(data_edges, axis=0).transpose()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "'''\n",
    "X, Y, webpage_id, edges = parse_wisconsin()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(webpage_id.shape)\n",
    "print(edges.shape)\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nX, Y, webpage_id, edges = parse_wisconsin()\\nprint(X.shape)\\nprint(Y.shape)\\nprint(webpage_id.shape)\\nprint(edges.shape)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "def get_webkb_casestudy(data_name, SEED=0):\n",
    "    data_X, data_Y, data_webpage_url, data_edges = parse_webkb(data_name)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)  # Numpy module.\n",
    "    random.seed(SEED)  # Python random module.\n",
    "\n",
    "    # load data\n",
    "    data = Data(x=torch.tensor(data_X).float(),\n",
    "                 edge_index=torch.tensor(data_edges).long(), \n",
    "                 y=torch.tensor(data_Y).long(),\n",
    "                 num_nodes=len(data_Y))\n",
    "    # split data\n",
    "    node_id = np.arange(data.num_nodes)\n",
    "    np.random.shuffle(node_id)\n",
    "\n",
    "    data.train_id = np.sort(node_id[:int(data.num_nodes * 0.6)])\n",
    "    data.val_id = np.sort(\n",
    "        node_id[int(data.num_nodes * 0.6):int(data.num_nodes * 0.8)])\n",
    "    data.test_id = np.sort(node_id[int(data.num_nodes * 0.8):])\n",
    "\n",
    "    data.train_mask = torch.tensor(\n",
    "        [x in data.train_id for x in range(data.num_nodes)])\n",
    "    data.val_mask = torch.tensor(\n",
    "        [x in data.val_id for x in range(data.num_nodes)])\n",
    "    data.test_mask = torch.tensor(\n",
    "        [x in data.test_id for x in range(data.num_nodes)])\n",
    "    \n",
    "    return data, data_webpage_url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "def html_process(input_string):\n",
    "    # 使用正则表达式去掉所有 HTML 标签\n",
    "    lines = input_string.split('\\n')\n",
    "    clean_text = ' '.join(lines[6:])\n",
    "\n",
    "    #non_empty_lines = [line for line in clean_text if line.strip()]\n",
    "    \n",
    "    #tag_list = ['<.*?>', r'<ahref\\s*=\\s*\".*?\"\\s*>', r'<a\\shref\\s*=\\s*\".*?\"\\s*>', r'<meta *.html>', r'<img src*>', r'<IMG SRC*\">', r'<bodyBACKGROUND*>', r'<imgsrc*>', r'<AHREF*>', '\\n']\n",
    "    tag_list = ['<.*?>', '\\n', r'<a\\s+href\\s*=\\s*\".*?\"\\s*>', r'<IMG\\s+SRC\\s*=\\s*\".*?\"\\s+ALT\\s*=\\s*\".*?\"\\s*>']\n",
    "    for tag in tag_list:\n",
    "        clean_text = re.sub(tag, '', clean_text, flags=re.IGNORECASE)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    return clean_text\n",
    "'''\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        try:\n",
    "            file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        except:\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            text.append('')\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text\n",
    "'''\n",
    "def delete_vacant_webpage(data, i):\n",
    "    data.y = torch.cat((data.y[:i], data.y[(i+1):]))\n",
    "    # data.edge_index = torch.cat((data.edge_index[:,:i], data.edge_index[:,(i+1):]), dim=1)\n",
    "    data.x = torch.cat((data.x[:i], data.x[(i+1):]))\n",
    "    data.train_mask = torch.cat((data.train_mask[:i], data.train_mask[(i+1):]))\n",
    "    data.val_mask = torch.cat((data.val_mask[:i], data.val_mask[(i+1):]))\n",
    "    data.test_mask = torch.cat((data.test_mask[:i], data.test_mask[(i+1):]))\n",
    "    data.train_id = np.delete(data.train_id, i)\n",
    "    data.val_id = np.delete(data.val_id, i)\n",
    "    data.test_id = np.delete(data.test_id, i)\n",
    "    data.num_nodes -= 1\n",
    "    mask = (data.edge_index[0] == i) | (data.edge_index[1] == i)\n",
    "    data.edge_index = data.edge_index[:,~mask] \n",
    "    return data\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        pages_to_remove = []\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "        if os.path.exists(file_path):\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        else:\n",
    "            pages_to_remove.append(i)\n",
    "            # print(i, file_path, 'not found') ###TODO\n",
    "            # text.append('')\n",
    "            \n",
    "    if data_name == 'wisconsin':\n",
    "        pages_to_remove = [3,5]\n",
    "   \n",
    "    for i in reversed(pages_to_remove):\n",
    "        print(i)\n",
    "        data = delete_vacant_webpage(data, i)\n",
    "        print(data)\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "data, clean_text = get_raw_text_webkb('wisconsin', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5\n",
      "Data(x=[264, 1703], edge_index=[2, 936], y=[264], num_nodes=264, train_id=[158], val_id=[52], test_id=[52], train_mask=[264], val_mask=[264], test_mask=[264])\n",
      "3\n",
      "Data(x=[263, 1703], edge_index=[2, 692], y=[263], num_nodes=263, train_id=[157], val_id=[51], test_id=[51], train_mask=[263], val_mask=[263], test_mask=[263])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_webkb(data_name):\n",
    "    path = f'/storage/qiaoyr/TAPE/dataset/web_kb/WebKB/{data_name}'\n",
    "    webpage_features_labels = np.genfromtxt(\"{}.content\".format(path), dtype=np.dtype(str))\n",
    "    data_X = webpage_features_labels[:, 1:-1].astype(np.float32)\n",
    "    labels = webpage_features_labels[:, -1]\n",
    "    #print(labels)\n",
    "    class_map = {x: i for i, x in enumerate(['course', 'faculty', 'student','project', 'staff'])}  \n",
    "    #print(class_map)\n",
    "    data_Y = np.array([class_map[x] for x in labels])\n",
    "    data_webpage_url = webpage_features_labels[:, 0]\n",
    "    # data_webpage_id = np.arange(len(data_webpage_url))\n",
    "    data_webpage_id_map = {x: i for i, x in enumerate(data_webpage_url)}\n",
    "    edges_unordered = np.genfromtxt(\"{}.cites\".format(path), dtype=np.dtype(str))\n",
    "    '''\n",
    "    for i in range(edges_unordered.shape[0]):\n",
    "        if edges_unordered[i][0] == edges_unordered[i][1]:\n",
    "            print('self loop:',edges_unordered[i][0])\n",
    "    '''\n",
    "    edges = np.array(list(map(data_webpage_id_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    #print(edges.shape)\n",
    "    data_edges = np.array(edges[~(edges == None).max(1)], dtype=np.int32)\n",
    "    #print(data_edges.shape)\n",
    "    data_edges = np.vstack((data_edges, np.fliplr(data_edges)))\n",
    "    #print(data_edges.shape)\n",
    "\n",
    "    return data_X, data_Y, data_webpage_url, np.unique(data_edges, axis=0).transpose()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "'''\n",
    "X, Y, webpage_id, edges = parse_wisconsin()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(webpage_id.shape)\n",
    "print(edges.shape)\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nX, Y, webpage_id, edges = parse_wisconsin()\\nprint(X.shape)\\nprint(Y.shape)\\nprint(webpage_id.shape)\\nprint(edges.shape)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "def get_webkb_casestudy(data_name, SEED=0):\n",
    "    data_X, data_Y, data_webpage_url, data_edges = parse_webkb(data_name)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)  # Numpy module.\n",
    "    random.seed(SEED)  # Python random module.\n",
    "\n",
    "    # load data\n",
    "    data = Data(x=torch.tensor(data_X).float(),\n",
    "                 edge_index=torch.tensor(data_edges).long(), \n",
    "                 y=torch.tensor(data_Y).long(),\n",
    "                 num_nodes=len(data_Y))\n",
    "    # split data\n",
    "    node_id = np.arange(data.num_nodes)\n",
    "    np.random.shuffle(node_id)\n",
    "\n",
    "    data.train_id = np.sort(node_id[:int(data.num_nodes * 0.6)])\n",
    "    data.val_id = np.sort(\n",
    "        node_id[int(data.num_nodes * 0.6):int(data.num_nodes * 0.8)])\n",
    "    data.test_id = np.sort(node_id[int(data.num_nodes * 0.8):])\n",
    "\n",
    "    data.train_mask = torch.tensor(\n",
    "        [x in data.train_id for x in range(data.num_nodes)])\n",
    "    data.val_mask = torch.tensor(\n",
    "        [x in data.val_id for x in range(data.num_nodes)])\n",
    "    data.test_mask = torch.tensor(\n",
    "        [x in data.test_id for x in range(data.num_nodes)])\n",
    "    \n",
    "    return data, data_webpage_url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "def html_process(input_string):\n",
    "    # 使用正则表达式去掉所有 HTML 标签\n",
    "    lines = input_string.split('\\n')\n",
    "    clean_text = ' '.join(lines[6:])\n",
    "\n",
    "    #non_empty_lines = [line for line in clean_text if line.strip()]\n",
    "    \n",
    "    #tag_list = ['<.*?>', r'<ahref\\s*=\\s*\".*?\"\\s*>', r'<a\\shref\\s*=\\s*\".*?\"\\s*>', r'<meta *.html>', r'<img src*>', r'<IMG SRC*\">', r'<bodyBACKGROUND*>', r'<imgsrc*>', r'<AHREF*>', '\\n']\n",
    "    tag_list = ['<.*?>', '\\n', r'<a\\s+href\\s*=\\s*\".*?\"\\s*>', r'<IMG\\s+SRC\\s*=\\s*\".*?\"\\s+ALT\\s*=\\s*\".*?\"\\s*>']\n",
    "    for tag in tag_list:\n",
    "        clean_text = re.sub(tag, '', clean_text, flags=re.IGNORECASE)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    return clean_text\n",
    "'''\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        try:\n",
    "            file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        except:\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            text.append('')\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text\n",
    "'''\n",
    "def delete_vacant_webpage(data, i):\n",
    "    data.y = torch.cat((data.y[:i], data.y[(i+1):]))\n",
    "    # data.edge_index = torch.cat((data.edge_index[:,:i], data.edge_index[:,(i+1):]), dim=1)\n",
    "    data.x = torch.cat((data.x[:i], data.x[(i+1):]))\n",
    "    data.train_mask = torch.cat((data.train_mask[:i], data.train_mask[(i+1):]))\n",
    "    data.val_mask = torch.cat((data.val_mask[:i], data.val_mask[(i+1):]))\n",
    "    data.test_mask = torch.cat((data.test_mask[:i], data.test_mask[(i+1):]))\n",
    "    data.train_id = np.delete(data.train_id, i)\n",
    "    data.val_id = np.delete(data.val_id, i)\n",
    "    data.test_id = np.delete(data.test_id, i)\n",
    "    data.num_nodes -= 1\n",
    "    mask = (data.edge_index[0] == i) | (data.edge_index[1] == i)\n",
    "    data.edge_index = data.edge_index[:,~mask] \n",
    "    return data\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        pages_to_remove = []\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "        if os.path.exists(file_path):\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        else:\n",
    "            pages_to_remove.append(i)\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            # text.append('')\n",
    "            \n",
    "    if data_name == 'wisconsin':\n",
    "        pages_to_remove = [3,5]\n",
    "   \n",
    "    for i in reversed(pages_to_remove):\n",
    "        data = delete_vacant_webpage(data, i)\n",
    "\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "data, clean_text = get_raw_text_webkb('cornell', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "12 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/cornell/http:^^www.cs.cornell.edu^ not found\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_webkb(data_name):\n",
    "    path = f'/storage/qiaoyr/TAPE/dataset/web_kb/WebKB/{data_name}'\n",
    "    webpage_features_labels = np.genfromtxt(\"{}.content\".format(path), dtype=np.dtype(str))\n",
    "    data_X = webpage_features_labels[:, 1:-1].astype(np.float32)\n",
    "    labels = webpage_features_labels[:, -1]\n",
    "    #print(labels)\n",
    "    class_map = {x: i for i, x in enumerate(['course', 'faculty', 'student','project', 'staff'])}  \n",
    "    #print(class_map)\n",
    "    data_Y = np.array([class_map[x] for x in labels])\n",
    "    data_webpage_url = webpage_features_labels[:, 0]\n",
    "    # data_webpage_id = np.arange(len(data_webpage_url))\n",
    "    data_webpage_id_map = {x: i for i, x in enumerate(data_webpage_url)}\n",
    "    edges_unordered = np.genfromtxt(\"{}.cites\".format(path), dtype=np.dtype(str))\n",
    "    '''\n",
    "    for i in range(edges_unordered.shape[0]):\n",
    "        if edges_unordered[i][0] == edges_unordered[i][1]:\n",
    "            print('self loop:',edges_unordered[i][0])\n",
    "    '''\n",
    "    edges = np.array(list(map(data_webpage_id_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    #print(edges.shape)\n",
    "    data_edges = np.array(edges[~(edges == None).max(1)], dtype=np.int32)\n",
    "    #print(data_edges.shape)\n",
    "    data_edges = np.vstack((data_edges, np.fliplr(data_edges)))\n",
    "    #print(data_edges.shape)\n",
    "\n",
    "    return data_X, data_Y, data_webpage_url, np.unique(data_edges, axis=0).transpose()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "'''\n",
    "X, Y, webpage_id, edges = parse_wisconsin()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(webpage_id.shape)\n",
    "print(edges.shape)\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nX, Y, webpage_id, edges = parse_wisconsin()\\nprint(X.shape)\\nprint(Y.shape)\\nprint(webpage_id.shape)\\nprint(edges.shape)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 116
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "def get_webkb_casestudy(data_name, SEED=0):\n",
    "    data_X, data_Y, data_webpage_url, data_edges = parse_webkb(data_name)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)  # Numpy module.\n",
    "    random.seed(SEED)  # Python random module.\n",
    "\n",
    "    # load data\n",
    "    data = Data(x=torch.tensor(data_X).float(),\n",
    "                 edge_index=torch.tensor(data_edges).long(), \n",
    "                 y=torch.tensor(data_Y).long(),\n",
    "                 num_nodes=len(data_Y))\n",
    "    # split data\n",
    "    node_id = np.arange(data.num_nodes)\n",
    "    np.random.shuffle(node_id)\n",
    "\n",
    "    data.train_id = np.sort(node_id[:int(data.num_nodes * 0.6)])\n",
    "    data.val_id = np.sort(\n",
    "        node_id[int(data.num_nodes * 0.6):int(data.num_nodes * 0.8)])\n",
    "    data.test_id = np.sort(node_id[int(data.num_nodes * 0.8):])\n",
    "\n",
    "    data.train_mask = torch.tensor(\n",
    "        [x in data.train_id for x in range(data.num_nodes)])\n",
    "    data.val_mask = torch.tensor(\n",
    "        [x in data.val_id for x in range(data.num_nodes)])\n",
    "    data.test_mask = torch.tensor(\n",
    "        [x in data.test_id for x in range(data.num_nodes)])\n",
    "    \n",
    "    return data, data_webpage_url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "def html_process(input_string):\n",
    "    # 使用正则表达式去掉所有 HTML 标签\n",
    "    lines = input_string.split('\\n')\n",
    "    clean_text = ' '.join(lines[6:])\n",
    "\n",
    "    #non_empty_lines = [line for line in clean_text if line.strip()]\n",
    "    \n",
    "    #tag_list = ['<.*?>', r'<ahref\\s*=\\s*\".*?\"\\s*>', r'<a\\shref\\s*=\\s*\".*?\"\\s*>', r'<meta *.html>', r'<img src*>', r'<IMG SRC*\">', r'<bodyBACKGROUND*>', r'<imgsrc*>', r'<AHREF*>', '\\n']\n",
    "    tag_list = ['<.*?>', '\\n', r'<a\\s+href\\s*=\\s*\".*?\"\\s*>', r'<IMG\\s+SRC\\s*=\\s*\".*?\"\\s+ALT\\s*=\\s*\".*?\"\\s*>']\n",
    "    for tag in tag_list:\n",
    "        clean_text = re.sub(tag, '', clean_text, flags=re.IGNORECASE)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    return clean_text\n",
    "'''\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        try:\n",
    "            file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        except:\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            text.append('')\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text\n",
    "'''\n",
    "def delete_vacant_webpage(data, i):\n",
    "    data.y = torch.cat((data.y[:i], data.y[(i+1):]))\n",
    "    # data.edge_index = torch.cat((data.edge_index[:,:i], data.edge_index[:,(i+1):]), dim=1)\n",
    "    data.x = torch.cat((data.x[:i], data.x[(i+1):]))\n",
    "    data.train_mask = torch.cat((data.train_mask[:i], data.train_mask[(i+1):]))\n",
    "    data.val_mask = torch.cat((data.val_mask[:i], data.val_mask[(i+1):]))\n",
    "    data.test_mask = torch.cat((data.test_mask[:i], data.test_mask[(i+1):]))\n",
    "    data.train_id = np.delete(data.train_id, i)\n",
    "    data.val_id = np.delete(data.val_id, i)\n",
    "    data.test_id = np.delete(data.test_id, i)\n",
    "    data.num_nodes -= 1\n",
    "    mask = (data.edge_index[0] == i) | (data.edge_index[1] == i)\n",
    "    data.edge_index = data.edge_index[:,~mask] \n",
    "    return data\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        pages_to_remove = []\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "        if os.path.exists(file_path):\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        else:\n",
    "            pages_to_remove.append(i)\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            # text.append('')\n",
    "            \n",
    "    if data_name == 'wisconsin':\n",
    "        pages_to_remove = [3,5]\n",
    "    elif data_name == 'cornell':\n",
    "        pages_to_remove = [12]\n",
    "   \n",
    "    for i in reversed(pages_to_remove):\n",
    "        data = delete_vacant_webpage(data, i)\n",
    "\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "data, clean_text = get_raw_text_webkb('cornell', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "12 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/cornell/http:^^www.cs.cornell.edu^ not found\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "print(data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data(x=[194, 1703], edge_index=[2, 381], y=[194], num_nodes=194, train_id=[116], val_id=[38], test_id=[38], train_mask=[194], val_mask=[194], test_mask=[194])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_webkb(data_name):\n",
    "    path = f'/storage/qiaoyr/TAPE/dataset/web_kb/WebKB/{data_name}'\n",
    "    webpage_features_labels = np.genfromtxt(\"{}.content\".format(path), dtype=np.dtype(str))\n",
    "    data_X = webpage_features_labels[:, 1:-1].astype(np.float32)\n",
    "    labels = webpage_features_labels[:, -1]\n",
    "    #print(labels)\n",
    "    class_map = {x: i for i, x in enumerate(['course', 'faculty', 'student','project', 'staff'])}  \n",
    "    #print(class_map)\n",
    "    data_Y = np.array([class_map[x] for x in labels])\n",
    "    data_webpage_url = webpage_features_labels[:, 0]\n",
    "    # data_webpage_id = np.arange(len(data_webpage_url))\n",
    "    data_webpage_id_map = {x: i for i, x in enumerate(data_webpage_url)}\n",
    "    edges_unordered = np.genfromtxt(\"{}.cites\".format(path), dtype=np.dtype(str))\n",
    "    '''\n",
    "    for i in range(edges_unordered.shape[0]):\n",
    "        if edges_unordered[i][0] == edges_unordered[i][1]:\n",
    "            print('self loop:',edges_unordered[i][0])\n",
    "    '''\n",
    "    edges = np.array(list(map(data_webpage_id_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    #print(edges.shape)\n",
    "    data_edges = np.array(edges[~(edges == None).max(1)], dtype=np.int32)\n",
    "    #print(data_edges.shape)\n",
    "    data_edges = np.vstack((data_edges, np.fliplr(data_edges)))\n",
    "    #print(data_edges.shape)\n",
    "\n",
    "    return data_X, data_Y, data_webpage_url, np.unique(data_edges, axis=0).transpose()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "'''\n",
    "X, Y, webpage_id, edges = parse_wisconsin()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(webpage_id.shape)\n",
    "print(edges.shape)\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nX, Y, webpage_id, edges = parse_wisconsin()\\nprint(X.shape)\\nprint(Y.shape)\\nprint(webpage_id.shape)\\nprint(edges.shape)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 122
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "def get_webkb_casestudy(data_name, SEED=0):\n",
    "    data_X, data_Y, data_webpage_url, data_edges = parse_webkb(data_name)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)  # Numpy module.\n",
    "    random.seed(SEED)  # Python random module.\n",
    "\n",
    "    # load data\n",
    "    data = Data(x=torch.tensor(data_X).float(),\n",
    "                 edge_index=torch.tensor(data_edges).long(), \n",
    "                 y=torch.tensor(data_Y).long(),\n",
    "                 num_nodes=len(data_Y))\n",
    "    # split data\n",
    "    node_id = np.arange(data.num_nodes)\n",
    "    np.random.shuffle(node_id)\n",
    "\n",
    "    data.train_id = np.sort(node_id[:int(data.num_nodes * 0.6)])\n",
    "    data.val_id = np.sort(\n",
    "        node_id[int(data.num_nodes * 0.6):int(data.num_nodes * 0.8)])\n",
    "    data.test_id = np.sort(node_id[int(data.num_nodes * 0.8):])\n",
    "\n",
    "    data.train_mask = torch.tensor(\n",
    "        [x in data.train_id for x in range(data.num_nodes)])\n",
    "    data.val_mask = torch.tensor(\n",
    "        [x in data.val_id for x in range(data.num_nodes)])\n",
    "    data.test_mask = torch.tensor(\n",
    "        [x in data.test_id for x in range(data.num_nodes)])\n",
    "    \n",
    "    return data, data_webpage_url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "def html_process(input_string):\n",
    "    # 使用正则表达式去掉所有 HTML 标签\n",
    "    lines = input_string.split('\\n')\n",
    "    clean_text = ' '.join(lines[6:])\n",
    "\n",
    "    #non_empty_lines = [line for line in clean_text if line.strip()]\n",
    "    \n",
    "    #tag_list = ['<.*?>', r'<ahref\\s*=\\s*\".*?\"\\s*>', r'<a\\shref\\s*=\\s*\".*?\"\\s*>', r'<meta *.html>', r'<img src*>', r'<IMG SRC*\">', r'<bodyBACKGROUND*>', r'<imgsrc*>', r'<AHREF*>', '\\n']\n",
    "    tag_list = ['<.*?>', '\\n', r'<a\\s+href\\s*=\\s*\".*?\"\\s*>', r'<IMG\\s+SRC\\s*=\\s*\".*?\"\\s+ALT\\s*=\\s*\".*?\"\\s*>']\n",
    "    for tag in tag_list:\n",
    "        clean_text = re.sub(tag, '', clean_text, flags=re.IGNORECASE)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    return clean_text\n",
    "'''\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        try:\n",
    "            file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        except:\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            text.append('')\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text\n",
    "'''\n",
    "def delete_vacant_webpage(data, i):\n",
    "    data.y = torch.cat((data.y[:i], data.y[(i+1):]))\n",
    "    # data.edge_index = torch.cat((data.edge_index[:,:i], data.edge_index[:,(i+1):]), dim=1)\n",
    "    data.x = torch.cat((data.x[:i], data.x[(i+1):]))\n",
    "    data.train_mask = torch.cat((data.train_mask[:i], data.train_mask[(i+1):]))\n",
    "    data.val_mask = torch.cat((data.val_mask[:i], data.val_mask[(i+1):]))\n",
    "    data.test_mask = torch.cat((data.test_mask[:i], data.test_mask[(i+1):]))\n",
    "    data.train_id = np.delete(data.train_id, i)\n",
    "    data.val_id = np.delete(data.val_id, i)\n",
    "    data.test_id = np.delete(data.test_id, i)\n",
    "    data.num_nodes -= 1\n",
    "    mask = (data.edge_index[0] == i) | (data.edge_index[1] == i)\n",
    "    data.edge_index = data.edge_index[:,~mask] \n",
    "    return data\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        pages_to_remove = []\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "        if os.path.exists(file_path):\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        else:\n",
    "            pages_to_remove.append(i)\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            # text.append('')\n",
    "            \n",
    "    if data_name == 'wisconsin':\n",
    "        pages_to_remove = [3,5]\n",
    "    elif data_name == 'cornell':\n",
    "        pages_to_remove = [12]\n",
    "   \n",
    "    for i in reversed(pages_to_remove):\n",
    "        print(i)\n",
    "        data = delete_vacant_webpage(data, i)\n",
    "        print(data)\n",
    "\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "data, clean_text = get_raw_text_webkb('cornell', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "12 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/cornell/http:^^www.cs.cornell.edu^ not found\n",
      "12\n",
      "Data(x=[194, 1703], edge_index=[2, 381], y=[194], num_nodes=194, train_id=[116], val_id=[38], test_id=[38], train_mask=[194], val_mask=[194], test_mask=[194])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_webkb(data_name):\n",
    "    path = f'/storage/qiaoyr/TAPE/dataset/web_kb/WebKB/{data_name}'\n",
    "    webpage_features_labels = np.genfromtxt(\"{}.content\".format(path), dtype=np.dtype(str))\n",
    "    data_X = webpage_features_labels[:, 1:-1].astype(np.float32)\n",
    "    labels = webpage_features_labels[:, -1]\n",
    "    #print(labels)\n",
    "    class_map = {x: i for i, x in enumerate(['course', 'faculty', 'student','project', 'staff'])}  \n",
    "    #print(class_map)\n",
    "    data_Y = np.array([class_map[x] for x in labels])\n",
    "    data_webpage_url = webpage_features_labels[:, 0]\n",
    "    # data_webpage_id = np.arange(len(data_webpage_url))\n",
    "    data_webpage_id_map = {x: i for i, x in enumerate(data_webpage_url)}\n",
    "    edges_unordered = np.genfromtxt(\"{}.cites\".format(path), dtype=np.dtype(str))\n",
    "    '''\n",
    "    for i in range(edges_unordered.shape[0]):\n",
    "        if edges_unordered[i][0] == edges_unordered[i][1]:\n",
    "            print('self loop:',edges_unordered[i][0])\n",
    "    '''\n",
    "    edges = np.array(list(map(data_webpage_id_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    #print(edges.shape)\n",
    "    data_edges = np.array(edges[~(edges == None).max(1)], dtype=np.int32)\n",
    "    #print(data_edges.shape)\n",
    "    data_edges = np.vstack((data_edges, np.fliplr(data_edges)))\n",
    "    #print(data_edges.shape)\n",
    "\n",
    "    return data_X, data_Y, data_webpage_url, np.unique(data_edges, axis=0).transpose()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "'''\n",
    "X, Y, webpage_id, edges = parse_wisconsin()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(webpage_id.shape)\n",
    "print(edges.shape)\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nX, Y, webpage_id, edges = parse_wisconsin()\\nprint(X.shape)\\nprint(Y.shape)\\nprint(webpage_id.shape)\\nprint(edges.shape)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 127
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "source": [
    "def get_webkb_casestudy(data_name, SEED=0):\n",
    "    data_X, data_Y, data_webpage_url, data_edges = parse_webkb(data_name)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)  # Numpy module.\n",
    "    random.seed(SEED)  # Python random module.\n",
    "\n",
    "    # load data\n",
    "    data = Data(x=torch.tensor(data_X).float(),\n",
    "                 edge_index=torch.tensor(data_edges).long(), \n",
    "                 y=torch.tensor(data_Y).long(),\n",
    "                 num_nodes=len(data_Y))\n",
    "    # split data\n",
    "    node_id = np.arange(data.num_nodes)\n",
    "    np.random.shuffle(node_id)\n",
    "\n",
    "    data.train_id = np.sort(node_id[:int(data.num_nodes * 0.6)])\n",
    "    data.val_id = np.sort(\n",
    "        node_id[int(data.num_nodes * 0.6):int(data.num_nodes * 0.8)])\n",
    "    data.test_id = np.sort(node_id[int(data.num_nodes * 0.8):])\n",
    "\n",
    "    data.train_mask = torch.tensor(\n",
    "        [x in data.train_id for x in range(data.num_nodes)])\n",
    "    data.val_mask = torch.tensor(\n",
    "        [x in data.val_id for x in range(data.num_nodes)])\n",
    "    data.test_mask = torch.tensor(\n",
    "        [x in data.test_id for x in range(data.num_nodes)])\n",
    "    \n",
    "    return data, data_webpage_url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "source": [
    "def html_process(input_string):\n",
    "    # 使用正则表达式去掉所有 HTML 标签\n",
    "    lines = input_string.split('\\n')\n",
    "    clean_text = ' '.join(lines[6:])\n",
    "\n",
    "    #non_empty_lines = [line for line in clean_text if line.strip()]\n",
    "    \n",
    "    #tag_list = ['<.*?>', r'<ahref\\s*=\\s*\".*?\"\\s*>', r'<a\\shref\\s*=\\s*\".*?\"\\s*>', r'<meta *.html>', r'<img src*>', r'<IMG SRC*\">', r'<bodyBACKGROUND*>', r'<imgsrc*>', r'<AHREF*>', '\\n']\n",
    "    tag_list = ['<.*?>', '\\n', r'<a\\s+href\\s*=\\s*\".*?\"\\s*>', r'<IMG\\s+SRC\\s*=\\s*\".*?\"\\s+ALT\\s*=\\s*\".*?\"\\s*>']\n",
    "    for tag in tag_list:\n",
    "        clean_text = re.sub(tag, '', clean_text, flags=re.IGNORECASE)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    return clean_text\n",
    "'''\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        try:\n",
    "            file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        except:\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            text.append('')\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text\n",
    "'''\n",
    "def delete_vacant_webpage(data, i):\n",
    "    data.y = torch.cat((data.y[:i], data.y[(i+1):]))\n",
    "    # data.edge_index = torch.cat((data.edge_index[:,:i], data.edge_index[:,(i+1):]), dim=1)\n",
    "    data.x = torch.cat((data.x[:i], data.x[(i+1):]))\n",
    "    data.train_mask = torch.cat((data.train_mask[:i], data.train_mask[(i+1):]))\n",
    "    data.val_mask = torch.cat((data.val_mask[:i], data.val_mask[(i+1):]))\n",
    "    data.test_mask = torch.cat((data.test_mask[:i], data.test_mask[(i+1):]))\n",
    "    data.train_id = np.delete(data.train_id, i)\n",
    "    data.val_id = np.delete(data.val_id, i)\n",
    "    data.test_id = np.delete(data.test_id, i)\n",
    "    data.num_nodes -= 1\n",
    "    mask = (data.edge_index[0] == i) | (data.edge_index[1] == i)\n",
    "    data.edge_index = data.edge_index[:,~mask] \n",
    "    return data\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        pages_to_remove = []\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "        if os.path.exists(file_path):\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        else:\n",
    "            pages_to_remove.append(i)\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            # text.append('')\n",
    "            \n",
    "    if data_name == 'wisconsin':\n",
    "        pages_to_remove = [3,5]\n",
    "    elif data_name == 'cornell':\n",
    "        pages_to_remove = [12]\n",
    "    print(data)\n",
    "    for i in reversed(pages_to_remove):\n",
    "        print(i)\n",
    "        data = delete_vacant_webpage(data, i)\n",
    "        print(data)\n",
    "\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "source": [
    "data, clean_text = get_raw_text_webkb('cornell', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "12 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/cornell/http:^^www.cs.cornell.edu^ not found\n",
      "Data(x=[195, 1703], edge_index=[2, 569], y=[195], num_nodes=195, train_id=[117], val_id=[39], test_id=[39], train_mask=[195], val_mask=[195], test_mask=[195])\n",
      "12\n",
      "Data(x=[194, 1703], edge_index=[2, 381], y=[194], num_nodes=194, train_id=[116], val_id=[38], test_id=[38], train_mask=[194], val_mask=[194], test_mask=[194])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "source": [
    "data, clean_text = get_raw_text_webkb('washington', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/washington/http:^^metacrawler.cs.washington.edu:8080^ not found\n",
      "1 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^ not found\n",
      "4 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^142^currentqtr^ not found\n",
      "5 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^143^currentqtr^ not found\n",
      "8 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^322^currentqtr^ not found\n",
      "14 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^370^currentqtr^ not found\n",
      "16 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^373^95a^index.html.95a^ not found\n",
      "18 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^378^currentqtr^ not found\n",
      "19 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^401^currentquarter^ not found\n",
      "24 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^451^currentquarter^ not found\n",
      "27 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^461^sp96^ not found\n",
      "29 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^467^fall96^ not found\n",
      "35 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^505^currentquarter^ not found\n",
      "36 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^505^fall94^ not found\n",
      "41 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^531^currentqtr^ not found\n",
      "51 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^590b^ not found\n",
      "53 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^590d^ not found\n",
      "54 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^590d^autumn95.html not found\n",
      "55 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^590d^autumn96.html not found\n",
      "63 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^cse370^currentqtr^ not found\n",
      "64 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^cse403^95w^ not found\n",
      "65 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^cse567^ not found\n",
      "70 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/washington/http:^^www.cs.washington.edu^homes^ahrens^devr^ not found\n",
      "110 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^gaetano^ not found\n",
      "152 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/student/washington/http:^^www.cs.washington.edu^homes^montgmry^ not found\n",
      "156 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^notkin^ not found\n",
      "170 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^ruzzo^ not found\n",
      "171 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^salesin^ not found\n",
      "178 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^shapiro^ not found\n",
      "214 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/washington/http:^^www.cs.washington.edu^research^community-networks^ not found\n",
      "226 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/washington/http:^^www.cs.washington.edu^research^projects^unisw^dyncomp^www^ not found\n",
      "227 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/washington/http:^^www.cs.washington.edu^research^projects^weird^www^ not found\n",
      "Data(x=[230, 1703], edge_index=[2, 783], y=[230], num_nodes=230, train_id=[138], val_id=[46], test_id=[46], train_mask=[230], val_mask=[230], test_mask=[230])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "source": [
    "data, clean_text = get_raw_text_webkb('texas', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/texas/http:^^www.cs.utexas.edu^ not found\n",
      "186 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/texas/http:^^www.ma.utexas.edu^users^bshults^atp^ not found\n",
      "Data(x=[187, 1703], edge_index=[2, 578], y=[187], num_nodes=187, train_id=[112], val_id=[37], test_id=[38], train_mask=[187], val_mask=[187], test_mask=[187])\n",
      "186\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 186 is out of bounds for axis 0 with size 112",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data, clean_text \u001b[39m=\u001b[39m get_raw_text_webkb(\u001b[39m'\u001b[39;49m\u001b[39mtexas\u001b[39;49m\u001b[39m'\u001b[39;49m, use_text\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, seed\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "\u001b[1;32m/storage/qiaoyr/TAPE/core/data_utils/load_webkb.py\u001b[0m in \u001b[0;36mline 91\u001b[0m, in \u001b[0;36mget_raw_text_webkb\u001b[0;34m(data_name, use_text, seed)\n\u001b[1;32m     <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=126'>127</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(pages_to_remove):\n\u001b[1;32m     <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=127'>128</a>\u001b[0m     \u001b[39mprint\u001b[39m(i)\n\u001b[0;32m---> <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=128'>129</a>\u001b[0m     data \u001b[39m=\u001b[39m delete_vacant_webpage(data, i)\n\u001b[1;32m     <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=129'>130</a>\u001b[0m     \u001b[39mprint\u001b[39m(data)\n\u001b[1;32m     <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=131'>132</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m text:\n",
      "\u001b[1;32m/storage/qiaoyr/TAPE/core/data_utils/load_webkb.py\u001b[0m in \u001b[0;36mline 51\u001b[0m, in \u001b[0;36mdelete_vacant_webpage\u001b[0;34m(data, i)\n\u001b[1;32m     <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=86'>87</a>\u001b[0m data\u001b[39m.\u001b[39mval_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((data\u001b[39m.\u001b[39mval_mask[:i], data\u001b[39m.\u001b[39mval_mask[(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):]))\n\u001b[1;32m     <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=87'>88</a>\u001b[0m data\u001b[39m.\u001b[39mtest_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((data\u001b[39m.\u001b[39mtest_mask[:i], data\u001b[39m.\u001b[39mtest_mask[(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):]))\n\u001b[0;32m---> <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=88'>89</a>\u001b[0m data\u001b[39m.\u001b[39mtrain_id \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdelete(data\u001b[39m.\u001b[39;49mtrain_id, i)\n\u001b[1;32m     <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=89'>90</a>\u001b[0m data\u001b[39m.\u001b[39mval_id \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdelete(data\u001b[39m.\u001b[39mval_id, i)\n\u001b[1;32m     <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=90'>91</a>\u001b[0m data\u001b[39m.\u001b[39mtest_id \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdelete(data\u001b[39m.\u001b[39mtest_id, i)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdelete\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py:5211\u001b[0m, in \u001b[0;36mdelete\u001b[0;34m(arr, obj, axis)\u001b[0m\n\u001b[1;32m   <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py?line=5207'>5208</a>\u001b[0m \u001b[39mif\u001b[39;00m single_value:\n\u001b[1;32m   <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py?line=5208'>5209</a>\u001b[0m     \u001b[39m# optimization for a single value\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py?line=5209'>5210</a>\u001b[0m     \u001b[39mif\u001b[39;00m (obj \u001b[39m<\u001b[39m \u001b[39m-\u001b[39mN \u001b[39mor\u001b[39;00m obj \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m N):\n\u001b[0;32m-> <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py?line=5210'>5211</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py?line=5211'>5212</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mindex \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m is out of bounds for axis \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py?line=5212'>5213</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39msize \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (obj, axis, N))\n\u001b[1;32m   <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py?line=5213'>5214</a>\u001b[0m     \u001b[39mif\u001b[39;00m (obj \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m   <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py?line=5214'>5215</a>\u001b[0m         obj \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m N\n",
      "\u001b[0;31mIndexError\u001b[0m: index 186 is out of bounds for axis 0 with size 112"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_webkb(data_name):\n",
    "    path = f'/storage/qiaoyr/TAPE/dataset/web_kb/WebKB/{data_name}'\n",
    "    webpage_features_labels = np.genfromtxt(\"{}.content\".format(path), dtype=np.dtype(str))\n",
    "    data_X = webpage_features_labels[:, 1:-1].astype(np.float32)\n",
    "    labels = webpage_features_labels[:, -1]\n",
    "    #print(labels)\n",
    "    class_map = {x: i for i, x in enumerate(['course', 'faculty', 'student','project', 'staff'])}  \n",
    "    #print(class_map)\n",
    "    data_Y = np.array([class_map[x] for x in labels])\n",
    "    data_webpage_url = webpage_features_labels[:, 0]\n",
    "    # data_webpage_id = np.arange(len(data_webpage_url))\n",
    "    data_webpage_id_map = {x: i for i, x in enumerate(data_webpage_url)}\n",
    "    edges_unordered = np.genfromtxt(\"{}.cites\".format(path), dtype=np.dtype(str))\n",
    "    '''\n",
    "    for i in range(edges_unordered.shape[0]):\n",
    "        if edges_unordered[i][0] == edges_unordered[i][1]:\n",
    "            print('self loop:',edges_unordered[i][0])\n",
    "    '''\n",
    "    edges = np.array(list(map(data_webpage_id_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    #print(edges.shape)\n",
    "    data_edges = np.array(edges[~(edges == None).max(1)], dtype=np.int32)\n",
    "    #print(data_edges.shape)\n",
    "    data_edges = np.vstack((data_edges, np.fliplr(data_edges)))\n",
    "    #print(data_edges.shape)\n",
    "\n",
    "    return data_X, data_Y, data_webpage_url, np.unique(data_edges, axis=0).transpose()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "source": [
    "'''\n",
    "X, Y, webpage_id, edges = parse_wisconsin()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(webpage_id.shape)\n",
    "print(edges.shape)\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nX, Y, webpage_id, edges = parse_wisconsin()\\nprint(X.shape)\\nprint(Y.shape)\\nprint(webpage_id.shape)\\nprint(edges.shape)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 134
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "source": [
    "def get_webkb_casestudy(data_name, SEED=0):\n",
    "    data_X, data_Y, data_webpage_url, data_edges = parse_webkb(data_name)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)  # Numpy module.\n",
    "    random.seed(SEED)  # Python random module.\n",
    "\n",
    "    # load data\n",
    "    data = Data(x=torch.tensor(data_X).float(),\n",
    "                 edge_index=torch.tensor(data_edges).long(), \n",
    "                 y=torch.tensor(data_Y).long(),\n",
    "                 num_nodes=len(data_Y))\n",
    "    # split data\n",
    "    node_id = np.arange(data.num_nodes)\n",
    "    np.random.shuffle(node_id)\n",
    "\n",
    "    data.train_id = np.sort(node_id[:int(data.num_nodes * 0.6)])\n",
    "    data.val_id = np.sort(\n",
    "        node_id[int(data.num_nodes * 0.6):int(data.num_nodes * 0.8)])\n",
    "    data.test_id = np.sort(node_id[int(data.num_nodes * 0.8):])\n",
    "\n",
    "    data.train_mask = torch.tensor(\n",
    "        [x in data.train_id for x in range(data.num_nodes)])\n",
    "    data.val_mask = torch.tensor(\n",
    "        [x in data.val_id for x in range(data.num_nodes)])\n",
    "    data.test_mask = torch.tensor(\n",
    "        [x in data.test_id for x in range(data.num_nodes)])\n",
    "    \n",
    "    return data, data_webpage_url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "source": [
    "def html_process(input_string):\n",
    "    # 使用正则表达式去掉所有 HTML 标签\n",
    "    lines = input_string.split('\\n')\n",
    "    clean_text = ' '.join(lines[6:])\n",
    "\n",
    "    #non_empty_lines = [line for line in clean_text if line.strip()]\n",
    "    \n",
    "    #tag_list = ['<.*?>', r'<ahref\\s*=\\s*\".*?\"\\s*>', r'<a\\shref\\s*=\\s*\".*?\"\\s*>', r'<meta *.html>', r'<img src*>', r'<IMG SRC*\">', r'<bodyBACKGROUND*>', r'<imgsrc*>', r'<AHREF*>', '\\n']\n",
    "    tag_list = ['<.*?>', '\\n', r'<a\\s+href\\s*=\\s*\".*?\"\\s*>', r'<IMG\\s+SRC\\s*=\\s*\".*?\"\\s+ALT\\s*=\\s*\".*?\"\\s*>']\n",
    "    for tag in tag_list:\n",
    "        clean_text = re.sub(tag, '', clean_text, flags=re.IGNORECASE)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    return clean_text\n",
    "'''\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        try:\n",
    "            file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        except:\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            text.append('')\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text\n",
    "'''\n",
    "def delete_vacant_webpage(data, i):\n",
    "    data.y = torch.cat((data.y[:i], data.y[(i+1):]))\n",
    "    # data.edge_index = torch.cat((data.edge_index[:,:i], data.edge_index[:,(i+1):]), dim=1)\n",
    "    data.x = torch.cat((data.x[:i], data.x[(i+1):]))\n",
    "    data.train_mask = torch.cat((data.train_mask[:i], data.train_mask[(i+1):]))\n",
    "    data.val_mask = torch.cat((data.val_mask[:i], data.val_mask[(i+1):]))\n",
    "    data.test_mask = torch.cat((data.test_mask[:i], data.test_mask[(i+1):]))\n",
    "    data.train_id = np.delete(data.train_id, i)\n",
    "    data.val_id = np.delete(data.val_id, i)\n",
    "    data.test_id = np.delete(data.test_id, i)\n",
    "    data.num_nodes -= 1\n",
    "    mask = (data.edge_index[0] == i) | (data.edge_index[1] == i)\n",
    "    data.edge_index = data.edge_index[:,~mask] \n",
    "    return data\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        pages_to_remove = []\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "        if os.path.exists(file_path):\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        else:\n",
    "            pages_to_remove.append(i)\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            # text.append('')\n",
    "            \n",
    "    if data_name == 'wisconsin':\n",
    "        pages_to_remove = [3,5]\n",
    "    elif data_name == 'cornell':\n",
    "        pages_to_remove = [12]\n",
    "    pages_to_remove = []\n",
    "    print(data)\n",
    "    for i in reversed(pages_to_remove):\n",
    "        print(i)\n",
    "        data = delete_vacant_webpage(data, i)\n",
    "        print(data)\n",
    "\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "source": [
    "data, clean_text = get_raw_text_webkb('texas', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/texas/http:^^www.cs.utexas.edu^ not found\n",
      "186 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/texas/http:^^www.ma.utexas.edu^users^bshults^atp^ not found\n",
      "Data(x=[187, 1703], edge_index=[2, 578], y=[187], num_nodes=187, train_id=[112], val_id=[37], test_id=[38], train_mask=[187], val_mask=[187], test_mask=[187])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_webkb(data_name):\n",
    "    path = f'/storage/qiaoyr/TAPE/dataset/web_kb/WebKB/{data_name}'\n",
    "    webpage_features_labels = np.genfromtxt(\"{}.content\".format(path), dtype=np.dtype(str))\n",
    "    data_X = webpage_features_labels[:, 1:-1].astype(np.float32)\n",
    "    labels = webpage_features_labels[:, -1]\n",
    "    #print(labels)\n",
    "    class_map = {x: i for i, x in enumerate(['course', 'faculty', 'student','project', 'staff'])}  \n",
    "    #print(class_map)\n",
    "    data_Y = np.array([class_map[x] for x in labels])\n",
    "    data_webpage_url = webpage_features_labels[:, 0]\n",
    "    # data_webpage_id = np.arange(len(data_webpage_url))\n",
    "    data_webpage_id_map = {x: i for i, x in enumerate(data_webpage_url)}\n",
    "    edges_unordered = np.genfromtxt(\"{}.cites\".format(path), dtype=np.dtype(str))\n",
    "    '''\n",
    "    for i in range(edges_unordered.shape[0]):\n",
    "        if edges_unordered[i][0] == edges_unordered[i][1]:\n",
    "            print('self loop:',edges_unordered[i][0])\n",
    "    '''\n",
    "    edges = np.array(list(map(data_webpage_id_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    #print(edges.shape)\n",
    "    data_edges = np.array(edges[~(edges == None).max(1)], dtype=np.int32)\n",
    "    #print(data_edges.shape)\n",
    "    data_edges = np.vstack((data_edges, np.fliplr(data_edges)))\n",
    "    #print(data_edges.shape)\n",
    "\n",
    "    return data_X, data_Y, data_webpage_url, np.unique(data_edges, axis=0).transpose()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "source": [
    "'''\n",
    "X, Y, webpage_id, edges = parse_wisconsin()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(webpage_id.shape)\n",
    "print(edges.shape)\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nX, Y, webpage_id, edges = parse_wisconsin()\\nprint(X.shape)\\nprint(Y.shape)\\nprint(webpage_id.shape)\\nprint(edges.shape)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 139
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "source": [
    "def get_webkb_casestudy(data_name, SEED=0):\n",
    "    data_X, data_Y, data_webpage_url, data_edges = parse_webkb(data_name)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)  # Numpy module.\n",
    "    random.seed(SEED)  # Python random module.\n",
    "\n",
    "    # load data\n",
    "    data = Data(x=torch.tensor(data_X).float(),\n",
    "                 edge_index=torch.tensor(data_edges).long(), \n",
    "                 y=torch.tensor(data_Y).long(),\n",
    "                 num_nodes=len(data_Y))\n",
    "    # split data\n",
    "    node_id = np.arange(data.num_nodes)\n",
    "    np.random.shuffle(node_id)\n",
    "\n",
    "    data.train_id = np.sort(node_id[:int(data.num_nodes * 0.6)])\n",
    "    data.val_id = np.sort(\n",
    "        node_id[int(data.num_nodes * 0.6):int(data.num_nodes * 0.8)])\n",
    "    data.test_id = np.sort(node_id[int(data.num_nodes * 0.8):])\n",
    "\n",
    "    data.train_mask = torch.tensor(\n",
    "        [x in data.train_id for x in range(data.num_nodes)])\n",
    "    data.val_mask = torch.tensor(\n",
    "        [x in data.val_id for x in range(data.num_nodes)])\n",
    "    data.test_mask = torch.tensor(\n",
    "        [x in data.test_id for x in range(data.num_nodes)])\n",
    "    \n",
    "    return data, data_webpage_url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "def html_process(input_string):\n",
    "    # 使用正则表达式去掉所有 HTML 标签\n",
    "    lines = input_string.split('\\n')\n",
    "    clean_text = ' '.join(lines[6:])\n",
    "\n",
    "    #non_empty_lines = [line for line in clean_text if line.strip()]\n",
    "    \n",
    "    #tag_list = ['<.*?>', r'<ahref\\s*=\\s*\".*?\"\\s*>', r'<a\\shref\\s*=\\s*\".*?\"\\s*>', r'<meta *.html>', r'<img src*>', r'<IMG SRC*\">', r'<bodyBACKGROUND*>', r'<imgsrc*>', r'<AHREF*>', '\\n']\n",
    "    tag_list = ['<.*?>', '\\n', r'<a\\s+href\\s*=\\s*\".*?\"\\s*>', r'<IMG\\s+SRC\\s*=\\s*\".*?\"\\s+ALT\\s*=\\s*\".*?\"\\s*>']\n",
    "    for tag in tag_list:\n",
    "        clean_text = re.sub(tag, '', clean_text, flags=re.IGNORECASE)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    return clean_text\n",
    "'''\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        try:\n",
    "            file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        except:\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            text.append('')\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text\n",
    "'''\n",
    "def delete_vacant_webpage(data, i):\n",
    "    data.y = torch.cat((data.y[:i], data.y[(i+1):]))\n",
    "    # data.edge_index = torch.cat((data.edge_index[:,:i], data.edge_index[:,(i+1):]), dim=1)\n",
    "    data.x = torch.cat((data.x[:i], data.x[(i+1):]))\n",
    "    data.train_mask = torch.cat((data.train_mask[:i], data.train_mask[(i+1):]))\n",
    "    data.val_mask = torch.cat((data.val_mask[:i], data.val_mask[(i+1):]))\n",
    "    data.test_mask = torch.cat((data.test_mask[:i], data.test_mask[(i+1):]))\n",
    "    data.train_id = np.delete(data.train_id, i)\n",
    "    data.val_id = np.delete(data.val_id, i)\n",
    "    data.test_id = np.delete(data.test_id, i)\n",
    "    data.num_nodes -= 1\n",
    "    mask = (data.edge_index[0] == i) | (data.edge_index[1] == i)\n",
    "    data.edge_index = data.edge_index[:,~mask] \n",
    "    return data\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        pages_to_remove = []\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "        if os.path.exists(file_path):\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        else:\n",
    "            pages_to_remove.append(i)\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            # text.append('')\n",
    "            \n",
    "    if data_name == 'wisconsin':\n",
    "        pages_to_remove = [3,5]\n",
    "    elif data_name == 'cornell':\n",
    "        pages_to_remove = [12]\n",
    "    pages_to_remove = []\n",
    "    print(data)\n",
    "    for i in reversed(pages_to_remove):\n",
    "        print(i)\n",
    "        data = delete_vacant_webpage(data, i)\n",
    "        print(data)\n",
    "\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "source": [
    "data, clean_text = get_raw_text_webkb('texas', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/texas/http:^^www.cs.utexas.edu^ not found\n",
      "Data(x=[187, 1703], edge_index=[2, 578], y=[187], num_nodes=187, train_id=[112], val_id=[37], test_id=[38], train_mask=[187], val_mask=[187], test_mask=[187])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_webkb(data_name):\n",
    "    path = f'/storage/qiaoyr/TAPE/dataset/web_kb/WebKB/{data_name}'\n",
    "    webpage_features_labels = np.genfromtxt(\"{}.content\".format(path), dtype=np.dtype(str))\n",
    "    data_X = webpage_features_labels[:, 1:-1].astype(np.float32)\n",
    "    labels = webpage_features_labels[:, -1]\n",
    "    #print(labels)\n",
    "    class_map = {x: i for i, x in enumerate(['course', 'faculty', 'student','project', 'staff'])}  \n",
    "    #print(class_map)\n",
    "    data_Y = np.array([class_map[x] for x in labels])\n",
    "    data_webpage_url = webpage_features_labels[:, 0]\n",
    "    # data_webpage_id = np.arange(len(data_webpage_url))\n",
    "    data_webpage_id_map = {x: i for i, x in enumerate(data_webpage_url)}\n",
    "    edges_unordered = np.genfromtxt(\"{}.cites\".format(path), dtype=np.dtype(str))\n",
    "    '''\n",
    "    for i in range(edges_unordered.shape[0]):\n",
    "        if edges_unordered[i][0] == edges_unordered[i][1]:\n",
    "            print('self loop:',edges_unordered[i][0])\n",
    "    '''\n",
    "    edges = np.array(list(map(data_webpage_id_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    #print(edges.shape)\n",
    "    data_edges = np.array(edges[~(edges == None).max(1)], dtype=np.int32)\n",
    "    #print(data_edges.shape)\n",
    "    data_edges = np.vstack((data_edges, np.fliplr(data_edges)))\n",
    "    #print(data_edges.shape)\n",
    "\n",
    "    return data_X, data_Y, data_webpage_url, np.unique(data_edges, axis=0).transpose()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "source": [
    "'''\n",
    "X, Y, webpage_id, edges = parse_wisconsin()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(webpage_id.shape)\n",
    "print(edges.shape)\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nX, Y, webpage_id, edges = parse_wisconsin()\\nprint(X.shape)\\nprint(Y.shape)\\nprint(webpage_id.shape)\\nprint(edges.shape)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 144
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "source": [
    "def get_webkb_casestudy(data_name, SEED=0):\n",
    "    data_X, data_Y, data_webpage_url, data_edges = parse_webkb(data_name)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)  # Numpy module.\n",
    "    random.seed(SEED)  # Python random module.\n",
    "\n",
    "    # load data\n",
    "    data = Data(x=torch.tensor(data_X).float(),\n",
    "                 edge_index=torch.tensor(data_edges).long(), \n",
    "                 y=torch.tensor(data_Y).long(),\n",
    "                 num_nodes=len(data_Y))\n",
    "    # split data\n",
    "    node_id = np.arange(data.num_nodes)\n",
    "    np.random.shuffle(node_id)\n",
    "\n",
    "    data.train_id = np.sort(node_id[:int(data.num_nodes * 0.6)])\n",
    "    data.val_id = np.sort(\n",
    "        node_id[int(data.num_nodes * 0.6):int(data.num_nodes * 0.8)])\n",
    "    data.test_id = np.sort(node_id[int(data.num_nodes * 0.8):])\n",
    "\n",
    "    data.train_mask = torch.tensor(\n",
    "        [x in data.train_id for x in range(data.num_nodes)])\n",
    "    data.val_mask = torch.tensor(\n",
    "        [x in data.val_id for x in range(data.num_nodes)])\n",
    "    data.test_mask = torch.tensor(\n",
    "        [x in data.test_id for x in range(data.num_nodes)])\n",
    "    \n",
    "    return data, data_webpage_url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "source": [
    "def html_process(input_string):\n",
    "    # 使用正则表达式去掉所有 HTML 标签\n",
    "    lines = input_string.split('\\n')\n",
    "    clean_text = ' '.join(lines[6:])\n",
    "\n",
    "    #non_empty_lines = [line for line in clean_text if line.strip()]\n",
    "    \n",
    "    #tag_list = ['<.*?>', r'<ahref\\s*=\\s*\".*?\"\\s*>', r'<a\\shref\\s*=\\s*\".*?\"\\s*>', r'<meta *.html>', r'<img src*>', r'<IMG SRC*\">', r'<bodyBACKGROUND*>', r'<imgsrc*>', r'<AHREF*>', '\\n']\n",
    "    tag_list = ['<.*?>', '\\n', r'<a\\s+href\\s*=\\s*\".*?\"\\s*>', r'<IMG\\s+SRC\\s*=\\s*\".*?\"\\s+ALT\\s*=\\s*\".*?\"\\s*>']\n",
    "    for tag in tag_list:\n",
    "        clean_text = re.sub(tag, '', clean_text, flags=re.IGNORECASE)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    return clean_text\n",
    "'''\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        try:\n",
    "            file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        except:\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            text.append('')\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text\n",
    "'''\n",
    "def delete_vacant_webpage(data, i):\n",
    "    data.y = torch.cat((data.y[:i], data.y[(i+1):]))\n",
    "    # data.edge_index = torch.cat((data.edge_index[:,:i], data.edge_index[:,(i+1):]), dim=1)\n",
    "    data.x = torch.cat((data.x[:i], data.x[(i+1):]))\n",
    "    data.train_mask = torch.cat((data.train_mask[:i], data.train_mask[(i+1):]))\n",
    "    data.val_mask = torch.cat((data.val_mask[:i], data.val_mask[(i+1):]))\n",
    "    data.test_mask = torch.cat((data.test_mask[:i], data.test_mask[(i+1):]))\n",
    "    data.train_id = np.delete(data.train_id, i)\n",
    "    data.val_id = np.delete(data.val_id, i)\n",
    "    data.test_id = np.delete(data.test_id, i)\n",
    "    data.num_nodes -= 1\n",
    "    mask = (data.edge_index[0] == i) | (data.edge_index[1] == i)\n",
    "    data.edge_index = data.edge_index[:,~mask] \n",
    "    return data\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        pages_to_remove = []\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "        if os.path.exists(file_path):\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        else:\n",
    "            pages_to_remove.append(i)\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            # text.append('')\n",
    "            \n",
    "    if data_name == 'wisconsin':\n",
    "        pages_to_remove = [3,5]\n",
    "    elif data_name == 'cornell':\n",
    "        pages_to_remove = [12]\n",
    "    elif data_name == 'texas':\n",
    "        pages_to_remove = [0]\n",
    "    pages_to_remove = []\n",
    "    print(data)\n",
    "    for i in reversed(pages_to_remove):\n",
    "        print(i)\n",
    "        data = delete_vacant_webpage(data, i)\n",
    "        print(data)\n",
    "\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "source": [
    "data, clean_text = get_raw_text_webkb('texas', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/texas/http:^^www.cs.utexas.edu^ not found\n",
      "Data(x=[187, 1703], edge_index=[2, 578], y=[187], num_nodes=187, train_id=[112], val_id=[37], test_id=[38], train_mask=[187], val_mask=[187], test_mask=[187])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_webkb(data_name):\n",
    "    path = f'/storage/qiaoyr/TAPE/dataset/web_kb/WebKB/{data_name}'\n",
    "    webpage_features_labels = np.genfromtxt(\"{}.content\".format(path), dtype=np.dtype(str))\n",
    "    data_X = webpage_features_labels[:, 1:-1].astype(np.float32)\n",
    "    labels = webpage_features_labels[:, -1]\n",
    "    #print(labels)\n",
    "    class_map = {x: i for i, x in enumerate(['course', 'faculty', 'student','project', 'staff'])}  \n",
    "    #print(class_map)\n",
    "    data_Y = np.array([class_map[x] for x in labels])\n",
    "    data_webpage_url = webpage_features_labels[:, 0]\n",
    "    # data_webpage_id = np.arange(len(data_webpage_url))\n",
    "    data_webpage_id_map = {x: i for i, x in enumerate(data_webpage_url)}\n",
    "    edges_unordered = np.genfromtxt(\"{}.cites\".format(path), dtype=np.dtype(str))\n",
    "    '''\n",
    "    for i in range(edges_unordered.shape[0]):\n",
    "        if edges_unordered[i][0] == edges_unordered[i][1]:\n",
    "            print('self loop:',edges_unordered[i][0])\n",
    "    '''\n",
    "    edges = np.array(list(map(data_webpage_id_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    #print(edges.shape)\n",
    "    data_edges = np.array(edges[~(edges == None).max(1)], dtype=np.int32)\n",
    "    #print(data_edges.shape)\n",
    "    data_edges = np.vstack((data_edges, np.fliplr(data_edges)))\n",
    "    #print(data_edges.shape)\n",
    "\n",
    "    return data_X, data_Y, data_webpage_url, np.unique(data_edges, axis=0).transpose()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "source": [
    "'''\n",
    "X, Y, webpage_id, edges = parse_wisconsin()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(webpage_id.shape)\n",
    "print(edges.shape)\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nX, Y, webpage_id, edges = parse_wisconsin()\\nprint(X.shape)\\nprint(Y.shape)\\nprint(webpage_id.shape)\\nprint(edges.shape)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 149
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "source": [
    "def get_webkb_casestudy(data_name, SEED=0):\n",
    "    data_X, data_Y, data_webpage_url, data_edges = parse_webkb(data_name)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)  # Numpy module.\n",
    "    random.seed(SEED)  # Python random module.\n",
    "\n",
    "    # load data\n",
    "    data = Data(x=torch.tensor(data_X).float(),\n",
    "                 edge_index=torch.tensor(data_edges).long(), \n",
    "                 y=torch.tensor(data_Y).long(),\n",
    "                 num_nodes=len(data_Y))\n",
    "    # split data\n",
    "    node_id = np.arange(data.num_nodes)\n",
    "    np.random.shuffle(node_id)\n",
    "\n",
    "    data.train_id = np.sort(node_id[:int(data.num_nodes * 0.6)])\n",
    "    data.val_id = np.sort(\n",
    "        node_id[int(data.num_nodes * 0.6):int(data.num_nodes * 0.8)])\n",
    "    data.test_id = np.sort(node_id[int(data.num_nodes * 0.8):])\n",
    "\n",
    "    data.train_mask = torch.tensor(\n",
    "        [x in data.train_id for x in range(data.num_nodes)])\n",
    "    data.val_mask = torch.tensor(\n",
    "        [x in data.val_id for x in range(data.num_nodes)])\n",
    "    data.test_mask = torch.tensor(\n",
    "        [x in data.test_id for x in range(data.num_nodes)])\n",
    "    \n",
    "    return data, data_webpage_url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "source": [
    "def html_process(input_string):\n",
    "    # 使用正则表达式去掉所有 HTML 标签\n",
    "    lines = input_string.split('\\n')\n",
    "    clean_text = ' '.join(lines[6:])\n",
    "\n",
    "    #non_empty_lines = [line for line in clean_text if line.strip()]\n",
    "    \n",
    "    #tag_list = ['<.*?>', r'<ahref\\s*=\\s*\".*?\"\\s*>', r'<a\\shref\\s*=\\s*\".*?\"\\s*>', r'<meta *.html>', r'<img src*>', r'<IMG SRC*\">', r'<bodyBACKGROUND*>', r'<imgsrc*>', r'<AHREF*>', '\\n']\n",
    "    tag_list = ['<.*?>', '\\n', r'<a\\s+href\\s*=\\s*\".*?\"\\s*>', r'<IMG\\s+SRC\\s*=\\s*\".*?\"\\s+ALT\\s*=\\s*\".*?\"\\s*>']\n",
    "    for tag in tag_list:\n",
    "        clean_text = re.sub(tag, '', clean_text, flags=re.IGNORECASE)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    return clean_text\n",
    "'''\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        try:\n",
    "            file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        except:\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            text.append('')\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text\n",
    "'''\n",
    "def delete_vacant_webpage(data, i):\n",
    "    data.y = torch.cat((data.y[:i], data.y[(i+1):]))\n",
    "    # data.edge_index = torch.cat((data.edge_index[:,:i], data.edge_index[:,(i+1):]), dim=1)\n",
    "    data.x = torch.cat((data.x[:i], data.x[(i+1):]))\n",
    "    data.train_mask = torch.cat((data.train_mask[:i], data.train_mask[(i+1):]))\n",
    "    data.val_mask = torch.cat((data.val_mask[:i], data.val_mask[(i+1):]))\n",
    "    data.test_mask = torch.cat((data.test_mask[:i], data.test_mask[(i+1):]))\n",
    "    data.train_id = np.delete(data.train_id, i)\n",
    "    data.val_id = np.delete(data.val_id, i)\n",
    "    data.test_id = np.delete(data.test_id, i)\n",
    "    data.num_nodes -= 1\n",
    "    mask = (data.edge_index[0] == i) | (data.edge_index[1] == i)\n",
    "    data.edge_index = data.edge_index[:,~mask] \n",
    "    return data\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        pages_to_remove = []\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "        if os.path.exists(file_path):\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        else:\n",
    "            pages_to_remove.append(i)\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            # text.append('')\n",
    "            \n",
    "    if data_name == 'wisconsin':\n",
    "        pages_to_remove = [3,5]\n",
    "    elif data_name == 'cornell':\n",
    "        pages_to_remove = [12]\n",
    "    elif data_name == 'texas':\n",
    "        pages_to_remove = [0]\n",
    "    # pages_to_remove = []\n",
    "    print(data)\n",
    "    for i in reversed(pages_to_remove):\n",
    "        print(i)\n",
    "        data = delete_vacant_webpage(data, i)\n",
    "        print(data)\n",
    "\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "source": [
    "data, clean_text = get_raw_text_webkb('texas', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/texas/http:^^www.cs.utexas.edu^ not found\n",
      "Data(x=[187, 1703], edge_index=[2, 578], y=[187], num_nodes=187, train_id=[112], val_id=[37], test_id=[38], train_mask=[187], val_mask=[187], test_mask=[187])\n",
      "0\n",
      "Data(x=[186, 1703], edge_index=[2, 370], y=[186], num_nodes=186, train_id=[111], val_id=[36], test_id=[37], train_mask=[186], val_mask=[186], test_mask=[186])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "source": [
    "data, clean_text = get_raw_text_webkb('washington', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/washington/http:^^metacrawler.cs.washington.edu:8080^ not found\n",
      "1 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^ not found\n",
      "4 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^142^currentqtr^ not found\n",
      "5 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^143^currentqtr^ not found\n",
      "8 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^322^currentqtr^ not found\n",
      "14 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^370^currentqtr^ not found\n",
      "16 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^373^95a^index.html.95a^ not found\n",
      "18 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^378^currentqtr^ not found\n",
      "19 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^401^currentquarter^ not found\n",
      "24 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^451^currentquarter^ not found\n",
      "27 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^461^sp96^ not found\n",
      "29 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^467^fall96^ not found\n",
      "35 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^505^currentquarter^ not found\n",
      "36 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^505^fall94^ not found\n",
      "41 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^531^currentqtr^ not found\n",
      "51 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^590b^ not found\n",
      "53 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^590d^ not found\n",
      "54 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^590d^autumn95.html not found\n",
      "55 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^590d^autumn96.html not found\n",
      "63 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^cse370^currentqtr^ not found\n",
      "64 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^cse403^95w^ not found\n",
      "65 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^education^courses^cse567^ not found\n",
      "70 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/washington/http:^^www.cs.washington.edu^homes^ahrens^devr^ not found\n",
      "110 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^gaetano^ not found\n",
      "152 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/student/washington/http:^^www.cs.washington.edu^homes^montgmry^ not found\n",
      "156 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^notkin^ not found\n",
      "170 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^ruzzo^ not found\n",
      "171 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^salesin^ not found\n",
      "178 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^shapiro^ not found\n",
      "214 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/washington/http:^^www.cs.washington.edu^research^community-networks^ not found\n",
      "226 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/washington/http:^^www.cs.washington.edu^research^projects^unisw^dyncomp^www^ not found\n",
      "227 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/washington/http:^^www.cs.washington.edu^research^projects^weird^www^ not found\n",
      "Data(x=[230, 1703], edge_index=[2, 783], y=[230], num_nodes=230, train_id=[138], val_id=[46], test_id=[46], train_mask=[230], val_mask=[230], test_mask=[230])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_webkb(data_name):\n",
    "    path = f'/storage/qiaoyr/TAPE/dataset/web_kb/WebKB/{data_name}'\n",
    "    webpage_features_labels = np.genfromtxt(\"{}.content\".format(path), dtype=np.dtype(str))\n",
    "    data_X = webpage_features_labels[:, 1:-1].astype(np.float32)\n",
    "    labels = webpage_features_labels[:, -1]\n",
    "    #print(labels)\n",
    "    class_map = {x: i for i, x in enumerate(['course', 'faculty', 'student','project', 'staff'])}  \n",
    "    #print(class_map)\n",
    "    data_Y = np.array([class_map[x] for x in labels])\n",
    "    data_webpage_url = webpage_features_labels[:, 0]\n",
    "    # data_webpage_id = np.arange(len(data_webpage_url))\n",
    "    data_webpage_id_map = {x: i for i, x in enumerate(data_webpage_url)}\n",
    "    edges_unordered = np.genfromtxt(\"{}.cites\".format(path), dtype=np.dtype(str))\n",
    "    '''\n",
    "    for i in range(edges_unordered.shape[0]):\n",
    "        if edges_unordered[i][0] == edges_unordered[i][1]:\n",
    "            print('self loop:',edges_unordered[i][0])\n",
    "    '''\n",
    "    edges = np.array(list(map(data_webpage_id_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    #print(edges.shape)\n",
    "    data_edges = np.array(edges[~(edges == None).max(1)], dtype=np.int32)\n",
    "    #print(data_edges.shape)\n",
    "    data_edges = np.vstack((data_edges, np.fliplr(data_edges)))\n",
    "    #print(data_edges.shape)\n",
    "\n",
    "    return data_X, data_Y, data_webpage_url, np.unique(data_edges, axis=0).transpose()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "source": [
    "'''\n",
    "X, Y, webpage_id, edges = parse_wisconsin()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(webpage_id.shape)\n",
    "print(edges.shape)\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nX, Y, webpage_id, edges = parse_wisconsin()\\nprint(X.shape)\\nprint(Y.shape)\\nprint(webpage_id.shape)\\nprint(edges.shape)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 155
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "source": [
    "def get_webkb_casestudy(data_name, SEED=0):\n",
    "    data_X, data_Y, data_webpage_url, data_edges = parse_webkb(data_name)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)  # Numpy module.\n",
    "    random.seed(SEED)  # Python random module.\n",
    "\n",
    "    # load data\n",
    "    data = Data(x=torch.tensor(data_X).float(),\n",
    "                 edge_index=torch.tensor(data_edges).long(), \n",
    "                 y=torch.tensor(data_Y).long(),\n",
    "                 num_nodes=len(data_Y))\n",
    "    # split data\n",
    "    node_id = np.arange(data.num_nodes)\n",
    "    np.random.shuffle(node_id)\n",
    "\n",
    "    data.train_id = np.sort(node_id[:int(data.num_nodes * 0.6)])\n",
    "    data.val_id = np.sort(\n",
    "        node_id[int(data.num_nodes * 0.6):int(data.num_nodes * 0.8)])\n",
    "    data.test_id = np.sort(node_id[int(data.num_nodes * 0.8):])\n",
    "\n",
    "    data.train_mask = torch.tensor(\n",
    "        [x in data.train_id for x in range(data.num_nodes)])\n",
    "    data.val_mask = torch.tensor(\n",
    "        [x in data.val_id for x in range(data.num_nodes)])\n",
    "    data.test_mask = torch.tensor(\n",
    "        [x in data.test_id for x in range(data.num_nodes)])\n",
    "    \n",
    "    return data, data_webpage_url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "source": [
    "def html_process(input_string):\n",
    "    # 使用正则表达式去掉所有 HTML 标签\n",
    "    lines = input_string.split('\\n')\n",
    "    clean_text = ' '.join(lines[6:])\n",
    "\n",
    "    #non_empty_lines = [line for line in clean_text if line.strip()]\n",
    "    \n",
    "    #tag_list = ['<.*?>', r'<ahref\\s*=\\s*\".*?\"\\s*>', r'<a\\shref\\s*=\\s*\".*?\"\\s*>', r'<meta *.html>', r'<img src*>', r'<IMG SRC*\">', r'<bodyBACKGROUND*>', r'<imgsrc*>', r'<AHREF*>', '\\n']\n",
    "    tag_list = ['<.*?>', '\\n', r'<a\\s+href\\s*=\\s*\".*?\"\\s*>', r'<IMG\\s+SRC\\s*=\\s*\".*?\"\\s+ALT\\s*=\\s*\".*?\"\\s*>']\n",
    "    for tag in tag_list:\n",
    "        clean_text = re.sub(tag, '', clean_text, flags=re.IGNORECASE)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    return clean_text\n",
    "'''\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        try:\n",
    "            file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        except:\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            text.append('')\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text\n",
    "'''\n",
    "def delete_vacant_webpage(data, i):\n",
    "    data.y = torch.cat((data.y[:i], data.y[(i+1):]))\n",
    "    # data.edge_index = torch.cat((data.edge_index[:,:i], data.edge_index[:,(i+1):]), dim=1)\n",
    "    data.x = torch.cat((data.x[:i], data.x[(i+1):]))\n",
    "    data.train_mask = torch.cat((data.train_mask[:i], data.train_mask[(i+1):]))\n",
    "    data.val_mask = torch.cat((data.val_mask[:i], data.val_mask[(i+1):]))\n",
    "    data.test_mask = torch.cat((data.test_mask[:i], data.test_mask[(i+1):]))\n",
    "    data.train_id = np.delete(data.train_id, i)\n",
    "    data.val_id = np.delete(data.val_id, i)\n",
    "    data.test_id = np.delete(data.test_id, i)\n",
    "    data.num_nodes -= 1\n",
    "    mask = (data.edge_index[0] == i) | (data.edge_index[1] == i)\n",
    "    data.edge_index = data.edge_index[:,~mask] \n",
    "    return data\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        pages_to_remove = []\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "        if os.path.exists(file_path):\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        else:\n",
    "            pages_to_remove.append(i)\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            # text.append('')\n",
    "            \n",
    "    if data_name == 'wisconsin':\n",
    "        pages_to_remove = [3,5]\n",
    "    elif data_name == 'cornell':\n",
    "        pages_to_remove = [12]\n",
    "    elif data_name == 'texas':\n",
    "        pages_to_remove = [0]\n",
    "    pages_to_remove = []\n",
    "    print(data)\n",
    "    for i in reversed(pages_to_remove):\n",
    "        print(i)\n",
    "        data = delete_vacant_webpage(data, i)\n",
    "        print(data)\n",
    "\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "source": [
    "data, clean_text = get_raw_text_webkb('washington', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^ not found\n",
      "152 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/student/washington/http:^^www.cs.washington.edu^homes^montgmry^ not found\n",
      "156 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^notkin^ not found\n",
      "170 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^ruzzo^ not found\n",
      "171 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^salesin^ not found\n",
      "178 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^shapiro^ not found\n",
      "214 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/washington/http:^^www.cs.washington.edu^research^community-networks^ not found\n",
      "227 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/washington/http:^^www.cs.washington.edu^research^projects^weird^www^ not found\n",
      "Data(x=[230, 1703], edge_index=[2, 783], y=[230], num_nodes=230, train_id=[138], val_id=[46], test_id=[46], train_mask=[230], val_mask=[230], test_mask=[230])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_webkb(data_name):\n",
    "    path = f'/storage/qiaoyr/TAPE/dataset/web_kb/WebKB/{data_name}'\n",
    "    webpage_features_labels = np.genfromtxt(\"{}.content\".format(path), dtype=np.dtype(str))\n",
    "    data_X = webpage_features_labels[:, 1:-1].astype(np.float32)\n",
    "    labels = webpage_features_labels[:, -1]\n",
    "    #print(labels)\n",
    "    class_map = {x: i for i, x in enumerate(['course', 'faculty', 'student','project', 'staff'])}  \n",
    "    #print(class_map)\n",
    "    data_Y = np.array([class_map[x] for x in labels])\n",
    "    data_webpage_url = webpage_features_labels[:, 0]\n",
    "    # data_webpage_id = np.arange(len(data_webpage_url))\n",
    "    data_webpage_id_map = {x: i for i, x in enumerate(data_webpage_url)}\n",
    "    edges_unordered = np.genfromtxt(\"{}.cites\".format(path), dtype=np.dtype(str))\n",
    "    '''\n",
    "    for i in range(edges_unordered.shape[0]):\n",
    "        if edges_unordered[i][0] == edges_unordered[i][1]:\n",
    "            print('self loop:',edges_unordered[i][0])\n",
    "    '''\n",
    "    edges = np.array(list(map(data_webpage_id_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    #print(edges.shape)\n",
    "    data_edges = np.array(edges[~(edges == None).max(1)], dtype=np.int32)\n",
    "    #print(data_edges.shape)\n",
    "    data_edges = np.vstack((data_edges, np.fliplr(data_edges)))\n",
    "    #print(data_edges.shape)\n",
    "\n",
    "    return data_X, data_Y, data_webpage_url, np.unique(data_edges, axis=0).transpose()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "source": [
    "'''\n",
    "X, Y, webpage_id, edges = parse_wisconsin()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(webpage_id.shape)\n",
    "print(edges.shape)\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nX, Y, webpage_id, edges = parse_wisconsin()\\nprint(X.shape)\\nprint(Y.shape)\\nprint(webpage_id.shape)\\nprint(edges.shape)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 160
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "source": [
    "def get_webkb_casestudy(data_name, SEED=0):\n",
    "    data_X, data_Y, data_webpage_url, data_edges = parse_webkb(data_name)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)  # Numpy module.\n",
    "    random.seed(SEED)  # Python random module.\n",
    "\n",
    "    # load data\n",
    "    data = Data(x=torch.tensor(data_X).float(),\n",
    "                 edge_index=torch.tensor(data_edges).long(), \n",
    "                 y=torch.tensor(data_Y).long(),\n",
    "                 num_nodes=len(data_Y))\n",
    "    # split data\n",
    "    node_id = np.arange(data.num_nodes)\n",
    "    np.random.shuffle(node_id)\n",
    "\n",
    "    data.train_id = np.sort(node_id[:int(data.num_nodes * 0.6)])\n",
    "    data.val_id = np.sort(\n",
    "        node_id[int(data.num_nodes * 0.6):int(data.num_nodes * 0.8)])\n",
    "    data.test_id = np.sort(node_id[int(data.num_nodes * 0.8):])\n",
    "\n",
    "    data.train_mask = torch.tensor(\n",
    "        [x in data.train_id for x in range(data.num_nodes)])\n",
    "    data.val_mask = torch.tensor(\n",
    "        [x in data.val_id for x in range(data.num_nodes)])\n",
    "    data.test_mask = torch.tensor(\n",
    "        [x in data.test_id for x in range(data.num_nodes)])\n",
    "    \n",
    "    return data, data_webpage_url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "source": [
    "def html_process(input_string):\n",
    "    # 使用正则表达式去掉所有 HTML 标签\n",
    "    lines = input_string.split('\\n')\n",
    "    clean_text = ' '.join(lines[6:])\n",
    "\n",
    "    #non_empty_lines = [line for line in clean_text if line.strip()]\n",
    "    \n",
    "    #tag_list = ['<.*?>', r'<ahref\\s*=\\s*\".*?\"\\s*>', r'<a\\shref\\s*=\\s*\".*?\"\\s*>', r'<meta *.html>', r'<img src*>', r'<IMG SRC*\">', r'<bodyBACKGROUND*>', r'<imgsrc*>', r'<AHREF*>', '\\n']\n",
    "    tag_list = ['<.*?>', '\\n', r'<a\\s+href\\s*=\\s*\".*?\"\\s*>', r'<IMG\\s+SRC\\s*=\\s*\".*?\"\\s+ALT\\s*=\\s*\".*?\"\\s*>']\n",
    "    for tag in tag_list:\n",
    "        clean_text = re.sub(tag, '', clean_text, flags=re.IGNORECASE)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    return clean_text\n",
    "'''\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        try:\n",
    "            file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        except:\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            text.append('')\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text\n",
    "'''\n",
    "def delete_vacant_webpage(data, i):\n",
    "    data.y = torch.cat((data.y[:i], data.y[(i+1):]))\n",
    "    # data.edge_index = torch.cat((data.edge_index[:,:i], data.edge_index[:,(i+1):]), dim=1)\n",
    "    data.x = torch.cat((data.x[:i], data.x[(i+1):]))\n",
    "    data.train_mask = torch.cat((data.train_mask[:i], data.train_mask[(i+1):]))\n",
    "    data.val_mask = torch.cat((data.val_mask[:i], data.val_mask[(i+1):]))\n",
    "    data.test_mask = torch.cat((data.test_mask[:i], data.test_mask[(i+1):]))\n",
    "    data.train_id = np.delete(data.train_id, i)\n",
    "    data.val_id = np.delete(data.val_id, i)\n",
    "    data.test_id = np.delete(data.test_id, i)\n",
    "    data.num_nodes -= 1\n",
    "    mask = (data.edge_index[0] == i) | (data.edge_index[1] == i)\n",
    "    data.edge_index = data.edge_index[:,~mask] \n",
    "    return data\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        pages_to_remove = []\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "        if os.path.exists(file_path):\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        else:\n",
    "            pages_to_remove.append(i)\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            # text.append('')\n",
    "            \n",
    "    if data_name == 'wisconsin':\n",
    "        pages_to_remove = [3,5]\n",
    "    elif data_name == 'cornell':\n",
    "        pages_to_remove = [12]\n",
    "    elif data_name == 'texas':\n",
    "        pages_to_remove = [0]\n",
    "    pages_to_remove = []\n",
    "    print(data)\n",
    "    for i in reversed(pages_to_remove):\n",
    "        print(i)\n",
    "        data = delete_vacant_webpage(data, i)\n",
    "        print(data)\n",
    "\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "source": [
    "data, clean_text = get_raw_text_webkb('washington', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^ not found\n",
      "152 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/student/washington/http:^^www.cs.washington.edu^homes^montgmry^ not found\n",
      "156 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^notkin^ not found\n",
      "170 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^ruzzo^ not found\n",
      "171 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^salesin^ not found\n",
      "178 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^shapiro^ not found\n",
      "214 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/washington/http:^^www.cs.washington.edu^research^community-networks^ not found\n",
      "227 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/washington/http:^^www.cs.washington.edu^research^projects^weird^www^ not found\n",
      "Data(x=[230, 1703], edge_index=[2, 783], y=[230], num_nodes=230, train_id=[138], val_id=[46], test_id=[46], train_mask=[230], val_mask=[230], test_mask=[230])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_webkb(data_name):\n",
    "    path = f'/storage/qiaoyr/TAPE/dataset/web_kb/WebKB/{data_name}'\n",
    "    webpage_features_labels = np.genfromtxt(\"{}.content\".format(path), dtype=np.dtype(str))\n",
    "    data_X = webpage_features_labels[:, 1:-1].astype(np.float32)\n",
    "    labels = webpage_features_labels[:, -1]\n",
    "    #print(labels)\n",
    "    class_map = {x: i for i, x in enumerate(['course', 'faculty', 'student','project', 'staff'])}  \n",
    "    #print(class_map)\n",
    "    data_Y = np.array([class_map[x] for x in labels])\n",
    "    data_webpage_url = webpage_features_labels[:, 0]\n",
    "    # data_webpage_id = np.arange(len(data_webpage_url))\n",
    "    data_webpage_id_map = {x: i for i, x in enumerate(data_webpage_url)}\n",
    "    edges_unordered = np.genfromtxt(\"{}.cites\".format(path), dtype=np.dtype(str))\n",
    "    '''\n",
    "    for i in range(edges_unordered.shape[0]):\n",
    "        if edges_unordered[i][0] == edges_unordered[i][1]:\n",
    "            print('self loop:',edges_unordered[i][0])\n",
    "    '''\n",
    "    edges = np.array(list(map(data_webpage_id_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    #print(edges.shape)\n",
    "    data_edges = np.array(edges[~(edges == None).max(1)], dtype=np.int32)\n",
    "    #print(data_edges.shape)\n",
    "    data_edges = np.vstack((data_edges, np.fliplr(data_edges)))\n",
    "    #print(data_edges.shape)\n",
    "\n",
    "    return data_X, data_Y, data_webpage_url, np.unique(data_edges, axis=0).transpose()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "source": [
    "'''\n",
    "X, Y, webpage_id, edges = parse_wisconsin()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(webpage_id.shape)\n",
    "print(edges.shape)\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nX, Y, webpage_id, edges = parse_wisconsin()\\nprint(X.shape)\\nprint(Y.shape)\\nprint(webpage_id.shape)\\nprint(edges.shape)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 165
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "source": [
    "def get_webkb_casestudy(data_name, SEED=0):\n",
    "    data_X, data_Y, data_webpage_url, data_edges = parse_webkb(data_name)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)  # Numpy module.\n",
    "    random.seed(SEED)  # Python random module.\n",
    "\n",
    "    # load data\n",
    "    data = Data(x=torch.tensor(data_X).float(),\n",
    "                 edge_index=torch.tensor(data_edges).long(), \n",
    "                 y=torch.tensor(data_Y).long(),\n",
    "                 num_nodes=len(data_Y))\n",
    "    # split data\n",
    "    node_id = np.arange(data.num_nodes)\n",
    "    np.random.shuffle(node_id)\n",
    "\n",
    "    data.train_id = np.sort(node_id[:int(data.num_nodes * 0.6)])\n",
    "    data.val_id = np.sort(\n",
    "        node_id[int(data.num_nodes * 0.6):int(data.num_nodes * 0.8)])\n",
    "    data.test_id = np.sort(node_id[int(data.num_nodes * 0.8):])\n",
    "\n",
    "    data.train_mask = torch.tensor(\n",
    "        [x in data.train_id for x in range(data.num_nodes)])\n",
    "    data.val_mask = torch.tensor(\n",
    "        [x in data.val_id for x in range(data.num_nodes)])\n",
    "    data.test_mask = torch.tensor(\n",
    "        [x in data.test_id for x in range(data.num_nodes)])\n",
    "    \n",
    "    return data, data_webpage_url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "source": [
    "def html_process(input_string):\n",
    "    # 使用正则表达式去掉所有 HTML 标签\n",
    "    lines = input_string.split('\\n')\n",
    "    clean_text = ' '.join(lines[6:])\n",
    "\n",
    "    #non_empty_lines = [line for line in clean_text if line.strip()]\n",
    "    \n",
    "    #tag_list = ['<.*?>', r'<ahref\\s*=\\s*\".*?\"\\s*>', r'<a\\shref\\s*=\\s*\".*?\"\\s*>', r'<meta *.html>', r'<img src*>', r'<IMG SRC*\">', r'<bodyBACKGROUND*>', r'<imgsrc*>', r'<AHREF*>', '\\n']\n",
    "    tag_list = ['<.*?>', '\\n', r'<a\\s+href\\s*=\\s*\".*?\"\\s*>', r'<IMG\\s+SRC\\s*=\\s*\".*?\"\\s+ALT\\s*=\\s*\".*?\"\\s*>']\n",
    "    for tag in tag_list:\n",
    "        clean_text = re.sub(tag, '', clean_text, flags=re.IGNORECASE)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    return clean_text\n",
    "'''\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        try:\n",
    "            file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        except:\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            text.append('')\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text\n",
    "'''\n",
    "def delete_vacant_webpage(data, i):\n",
    "    data.y = torch.cat((data.y[:i], data.y[(i+1):]))\n",
    "    # data.edge_index = torch.cat((data.edge_index[:,:i], data.edge_index[:,(i+1):]), dim=1)\n",
    "    data.x = torch.cat((data.x[:i], data.x[(i+1):]))\n",
    "    data.train_mask = torch.cat((data.train_mask[:i], data.train_mask[(i+1):]))\n",
    "    data.val_mask = torch.cat((data.val_mask[:i], data.val_mask[(i+1):]))\n",
    "    data.test_mask = torch.cat((data.test_mask[:i], data.test_mask[(i+1):]))\n",
    "    data.train_id = np.delete(data.train_id, i)\n",
    "    data.val_id = np.delete(data.val_id, i)\n",
    "    data.test_id = np.delete(data.test_id, i)\n",
    "    data.num_nodes -= 1\n",
    "    mask = (data.edge_index[0] == i) | (data.edge_index[1] == i)\n",
    "    data.edge_index = data.edge_index[:,~mask] \n",
    "    return data\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        pages_to_remove = []\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "        if os.path.exists(file_path):\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        else:\n",
    "            pages_to_remove.append(i)\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            # text.append('')\n",
    "            \n",
    "    if data_name == 'wisconsin':\n",
    "        pages_to_remove = [3,5]\n",
    "    elif data_name == 'cornell':\n",
    "        pages_to_remove = [12]\n",
    "    elif data_name == 'texas':\n",
    "        pages_to_remove = [0]\n",
    "    pages_to_remove = []\n",
    "    print(data)\n",
    "    for i in reversed(pages_to_remove):\n",
    "        print(i)\n",
    "        data = delete_vacant_webpage(data, i)\n",
    "        print(data)\n",
    "\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "source": [
    "data, clean_text = get_raw_text_webkb('washington', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^ not found\n",
      "152 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/student/washington/http:^^www.cs.washington.edu^homes^montgmry^ not found\n",
      "156 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^notkin^ not found\n",
      "170 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^ruzzo^ not found\n",
      "171 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^salesin^ not found\n",
      "178 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^shapiro^ not found\n",
      "214 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/washington/http:^^www.cs.washington.edu^research^community-networks^ not found\n",
      "227 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/washington/http:^^www.cs.washington.edu^research^projects^weird^www^ not found\n",
      "Data(x=[230, 1703], edge_index=[2, 783], y=[230], num_nodes=230, train_id=[138], val_id=[46], test_id=[46], train_mask=[230], val_mask=[230], test_mask=[230])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "source": [
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        pages_to_remove = []\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "        if os.path.exists(file_path):\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        else:\n",
    "            pages_to_remove.append(i)\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            # text.append('')\n",
    "            \n",
    "    if data_name == 'wisconsin':\n",
    "        pages_to_remove = [3,5]\n",
    "    elif data_name == 'cornell':\n",
    "        pages_to_remove = [12]\n",
    "    elif data_name == 'texas':\n",
    "        pages_to_remove = [0]\n",
    "    elif data_name == 'washington':\n",
    "        pages_to_remove = [1, 152, 156,170,171,178,214,227]\n",
    "    print(data)\n",
    "    for i in reversed(pages_to_remove):\n",
    "        print(i)\n",
    "        data = delete_vacant_webpage(data, i)\n",
    "        print(data)\n",
    "\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "source": [
    "data, clean_text = get_raw_text_webkb('washington', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^ not found\n",
      "152 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/student/washington/http:^^www.cs.washington.edu^homes^montgmry^ not found\n",
      "156 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^notkin^ not found\n",
      "170 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^ruzzo^ not found\n",
      "171 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^salesin^ not found\n",
      "178 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^shapiro^ not found\n",
      "214 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/washington/http:^^www.cs.washington.edu^research^community-networks^ not found\n",
      "227 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/washington/http:^^www.cs.washington.edu^research^projects^weird^www^ not found\n",
      "Data(x=[230, 1703], edge_index=[2, 783], y=[230], num_nodes=230, train_id=[138], val_id=[46], test_id=[46], train_mask=[230], val_mask=[230], test_mask=[230])\n",
      "227\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 227 is out of bounds for axis 0 with size 138",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[170], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data, clean_text \u001b[39m=\u001b[39m get_raw_text_webkb(\u001b[39m'\u001b[39;49m\u001b[39mwashington\u001b[39;49m\u001b[39m'\u001b[39;49m, use_text\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, seed\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[169], line 39\u001b[0m, in \u001b[0;36mget_raw_text_webkb\u001b[0;34m(data_name, use_text, seed)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(pages_to_remove):\n\u001b[1;32m     38\u001b[0m     \u001b[39mprint\u001b[39m(i)\n\u001b[0;32m---> 39\u001b[0m     data \u001b[39m=\u001b[39m delete_vacant_webpage(data, i)\n\u001b[1;32m     40\u001b[0m     \u001b[39mprint\u001b[39m(data)\n\u001b[1;32m     42\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m text:\n",
      "\u001b[1;32m/storage/qiaoyr/TAPE/core/data_utils/load_webkb.py\u001b[0m in \u001b[0;36mline 51\u001b[0m, in \u001b[0;36mdelete_vacant_webpage\u001b[0;34m(data, i)\n\u001b[1;32m     <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=86'>87</a>\u001b[0m data\u001b[39m.\u001b[39mval_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((data\u001b[39m.\u001b[39mval_mask[:i], data\u001b[39m.\u001b[39mval_mask[(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):]))\n\u001b[1;32m     <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=87'>88</a>\u001b[0m data\u001b[39m.\u001b[39mtest_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((data\u001b[39m.\u001b[39mtest_mask[:i], data\u001b[39m.\u001b[39mtest_mask[(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):]))\n\u001b[0;32m---> <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=88'>89</a>\u001b[0m data\u001b[39m.\u001b[39mtrain_id \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdelete(data\u001b[39m.\u001b[39;49mtrain_id, i)\n\u001b[1;32m     <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=89'>90</a>\u001b[0m data\u001b[39m.\u001b[39mval_id \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdelete(data\u001b[39m.\u001b[39mval_id, i)\n\u001b[1;32m     <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=90'>91</a>\u001b[0m data\u001b[39m.\u001b[39mtest_id \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdelete(data\u001b[39m.\u001b[39mtest_id, i)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdelete\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py:5211\u001b[0m, in \u001b[0;36mdelete\u001b[0;34m(arr, obj, axis)\u001b[0m\n\u001b[1;32m   <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py?line=5207'>5208</a>\u001b[0m \u001b[39mif\u001b[39;00m single_value:\n\u001b[1;32m   <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py?line=5208'>5209</a>\u001b[0m     \u001b[39m# optimization for a single value\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py?line=5209'>5210</a>\u001b[0m     \u001b[39mif\u001b[39;00m (obj \u001b[39m<\u001b[39m \u001b[39m-\u001b[39mN \u001b[39mor\u001b[39;00m obj \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m N):\n\u001b[0;32m-> <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py?line=5210'>5211</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py?line=5211'>5212</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mindex \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m is out of bounds for axis \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py?line=5212'>5213</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39msize \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (obj, axis, N))\n\u001b[1;32m   <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py?line=5213'>5214</a>\u001b[0m     \u001b[39mif\u001b[39;00m (obj \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m   <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py?line=5214'>5215</a>\u001b[0m         obj \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m N\n",
      "\u001b[0;31mIndexError\u001b[0m: index 227 is out of bounds for axis 0 with size 138"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "source": [
    "data, clean_text = get_raw_text_webkb('wisconsin', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/wisconsin/http:^^www.cs.wisc.edu^ not found\n",
      "5 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/wisconsin/http:^^www.cs.wisc.edu^condor^next.html not found\n",
      "Data(x=[265, 1703], edge_index=[2, 938], y=[265], num_nodes=265, train_id=[159], val_id=[53], test_id=[53], train_mask=[265], val_mask=[265], test_mask=[265])\n",
      "5\n",
      "Data(x=[264, 1703], edge_index=[2, 936], y=[264], num_nodes=264, train_id=[158], val_id=[52], test_id=[52], train_mask=[264], val_mask=[264], test_mask=[264])\n",
      "3\n",
      "Data(x=[263, 1703], edge_index=[2, 692], y=[263], num_nodes=263, train_id=[157], val_id=[51], test_id=[51], train_mask=[263], val_mask=[263], test_mask=[263])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "source": [
    "data, clean_text = get_raw_text_webkb('washington', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^ not found\n",
      "152 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/student/washington/http:^^www.cs.washington.edu^homes^montgmry^ not found\n",
      "156 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^notkin^ not found\n",
      "170 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^ruzzo^ not found\n",
      "171 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^salesin^ not found\n",
      "178 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^shapiro^ not found\n",
      "214 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/washington/http:^^www.cs.washington.edu^research^community-networks^ not found\n",
      "227 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/washington/http:^^www.cs.washington.edu^research^projects^weird^www^ not found\n",
      "Data(x=[230, 1703], edge_index=[2, 783], y=[230], num_nodes=230, train_id=[138], val_id=[46], test_id=[46], train_mask=[230], val_mask=[230], test_mask=[230])\n",
      "227\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 227 is out of bounds for axis 0 with size 138",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[172], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data, clean_text \u001b[39m=\u001b[39m get_raw_text_webkb(\u001b[39m'\u001b[39;49m\u001b[39mwashington\u001b[39;49m\u001b[39m'\u001b[39;49m, use_text\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, seed\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[169], line 39\u001b[0m, in \u001b[0;36mget_raw_text_webkb\u001b[0;34m(data_name, use_text, seed)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(pages_to_remove):\n\u001b[1;32m     38\u001b[0m     \u001b[39mprint\u001b[39m(i)\n\u001b[0;32m---> 39\u001b[0m     data \u001b[39m=\u001b[39m delete_vacant_webpage(data, i)\n\u001b[1;32m     40\u001b[0m     \u001b[39mprint\u001b[39m(data)\n\u001b[1;32m     42\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m text:\n",
      "\u001b[1;32m/storage/qiaoyr/TAPE/core/data_utils/load_webkb.py\u001b[0m in \u001b[0;36mline 51\u001b[0m, in \u001b[0;36mdelete_vacant_webpage\u001b[0;34m(data, i)\n\u001b[1;32m     <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=86'>87</a>\u001b[0m data\u001b[39m.\u001b[39mval_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((data\u001b[39m.\u001b[39mval_mask[:i], data\u001b[39m.\u001b[39mval_mask[(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):]))\n\u001b[1;32m     <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=87'>88</a>\u001b[0m data\u001b[39m.\u001b[39mtest_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((data\u001b[39m.\u001b[39mtest_mask[:i], data\u001b[39m.\u001b[39mtest_mask[(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):]))\n\u001b[0;32m---> <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=88'>89</a>\u001b[0m data\u001b[39m.\u001b[39mtrain_id \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdelete(data\u001b[39m.\u001b[39;49mtrain_id, i)\n\u001b[1;32m     <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=89'>90</a>\u001b[0m data\u001b[39m.\u001b[39mval_id \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdelete(data\u001b[39m.\u001b[39mval_id, i)\n\u001b[1;32m     <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=90'>91</a>\u001b[0m data\u001b[39m.\u001b[39mtest_id \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdelete(data\u001b[39m.\u001b[39mtest_id, i)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdelete\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py:5211\u001b[0m, in \u001b[0;36mdelete\u001b[0;34m(arr, obj, axis)\u001b[0m\n\u001b[1;32m   <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py?line=5207'>5208</a>\u001b[0m \u001b[39mif\u001b[39;00m single_value:\n\u001b[1;32m   <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py?line=5208'>5209</a>\u001b[0m     \u001b[39m# optimization for a single value\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py?line=5209'>5210</a>\u001b[0m     \u001b[39mif\u001b[39;00m (obj \u001b[39m<\u001b[39m \u001b[39m-\u001b[39mN \u001b[39mor\u001b[39;00m obj \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m N):\n\u001b[0;32m-> <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py?line=5210'>5211</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py?line=5211'>5212</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mindex \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m is out of bounds for axis \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py?line=5212'>5213</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39msize \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (obj, axis, N))\n\u001b[1;32m   <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py?line=5213'>5214</a>\u001b[0m     \u001b[39mif\u001b[39;00m (obj \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m   <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/numpy/lib/function_base.py?line=5214'>5215</a>\u001b[0m         obj \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m N\n",
      "\u001b[0;31mIndexError\u001b[0m: index 227 is out of bounds for axis 0 with size 138"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "source": [
    "def delete_vacant_webpage(data, i):\n",
    "    data.y = torch.cat((data.y[:i], data.y[(i+1):]))\n",
    "    # data.edge_index = torch.cat((data.edge_index[:,:i], data.edge_index[:,(i+1):]), dim=1)\n",
    "    data.x = torch.cat((data.x[:i], data.x[(i+1):]))\n",
    "    data.train_mask = torch.cat((data.train_mask[:i], data.train_mask[(i+1):]))\n",
    "    data.val_mask = torch.cat((data.val_mask[:i], data.val_mask[(i+1):]))\n",
    "    data.test_mask = torch.cat((data.test_mask[:i], data.test_mask[(i+1):]))\n",
    "    # data.train_id = np.delete(data.train_id, i)\n",
    "    data.train_id = np.array(data.train_mask.nonzero().flatten())\n",
    "    data.val_id = np.array(data.val_mask.nonzero().flatten())\n",
    "    data.test_id = np.array(data.test_mask.nonzero().flatten())\n",
    "    data.num_nodes -= 1\n",
    "    mask = (data.edge_index[0] == i) | (data.edge_index[1] == i)\n",
    "    data.edge_index = data.edge_index[:,~mask] \n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "source": [
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        pages_to_remove = []\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "        if os.path.exists(file_path):\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        else:\n",
    "            pages_to_remove.append(i)\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            # text.append('')\n",
    "            \n",
    "    if data_name == 'wisconsin':\n",
    "        pages_to_remove = [3,5]\n",
    "    elif data_name == 'cornell':\n",
    "        pages_to_remove = [12]\n",
    "    elif data_name == 'texas':\n",
    "        pages_to_remove = [0]\n",
    "    elif data_name == 'washington':\n",
    "        pages_to_remove = [1, 152, 156,170,171,178,214,227]\n",
    "    print(data)\n",
    "    for i in reversed(pages_to_remove):\n",
    "        print(i)\n",
    "        data = delete_vacant_webpage(data, i)\n",
    "        print(data)\n",
    "\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "source": [
    "data, clean_text = get_raw_text_webkb('washington', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/washington/http:^^www.cs.washington.edu^ not found\n",
      "152 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/student/washington/http:^^www.cs.washington.edu^homes^montgmry^ not found\n",
      "156 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^notkin^ not found\n",
      "170 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^ruzzo^ not found\n",
      "171 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^salesin^ not found\n",
      "178 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/faculty/washington/http:^^www.cs.washington.edu^homes^shapiro^ not found\n",
      "214 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/washington/http:^^www.cs.washington.edu^research^community-networks^ not found\n",
      "227 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/washington/http:^^www.cs.washington.edu^research^projects^weird^www^ not found\n",
      "Data(x=[230, 1703], edge_index=[2, 783], y=[230], num_nodes=230, train_id=[138], val_id=[46], test_id=[46], train_mask=[230], val_mask=[230], test_mask=[230])\n",
      "227\n",
      "Data(x=[229, 1703], edge_index=[2, 782], y=[229], num_nodes=229, train_id=[138], val_id=[45], test_id=[46], train_mask=[229], val_mask=[229], test_mask=[229])\n",
      "214\n",
      "Data(x=[228, 1703], edge_index=[2, 781], y=[228], num_nodes=228, train_id=[138], val_id=[44], test_id=[46], train_mask=[228], val_mask=[228], test_mask=[228])\n",
      "178\n",
      "Data(x=[227, 1703], edge_index=[2, 778], y=[227], num_nodes=227, train_id=[137], val_id=[44], test_id=[46], train_mask=[227], val_mask=[227], test_mask=[227])\n",
      "171\n",
      "Data(x=[226, 1703], edge_index=[2, 773], y=[226], num_nodes=226, train_id=[136], val_id=[44], test_id=[46], train_mask=[226], val_mask=[226], test_mask=[226])\n",
      "170\n",
      "Data(x=[225, 1703], edge_index=[2, 768], y=[225], num_nodes=225, train_id=[135], val_id=[44], test_id=[46], train_mask=[225], val_mask=[225], test_mask=[225])\n",
      "156\n",
      "Data(x=[224, 1703], edge_index=[2, 763], y=[224], num_nodes=224, train_id=[134], val_id=[44], test_id=[46], train_mask=[224], val_mask=[224], test_mask=[224])\n",
      "152\n",
      "Data(x=[223, 1703], edge_index=[2, 762], y=[223], num_nodes=223, train_id=[133], val_id=[44], test_id=[46], train_mask=[223], val_mask=[223], test_mask=[223])\n",
      "1\n",
      "Data(x=[222, 1703], edge_index=[2, 518], y=[222], num_nodes=222, train_id=[133], val_id=[43], test_id=[46], train_mask=[222], val_mask=[222], test_mask=[222])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "source": [
    "print(data)\n",
    "print(len(clean_text))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data(x=[222, 1703], edge_index=[2, 518], y=[222], num_nodes=222, train_id=[133], val_id=[43], test_id=[46], train_mask=[222], val_mask=[222], test_mask=[222])\n",
      "222\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "source": [
    "data, clean_text = get_raw_text_webkb('wisconsin', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/wisconsin/http:^^www.cs.wisc.edu^ not found\n",
      "5 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/wisconsin/http:^^www.cs.wisc.edu^condor^next.html not found\n",
      "Data(x=[265, 1703], edge_index=[2, 938], y=[265], num_nodes=265, train_id=[159], val_id=[53], test_id=[53], train_mask=[265], val_mask=[265], test_mask=[265])\n",
      "5\n",
      "Data(x=[264, 1703], edge_index=[2, 936], y=[264], num_nodes=264, train_id=[158], val_id=[53], test_id=[53], train_mask=[264], val_mask=[264], test_mask=[264])\n",
      "3\n",
      "Data(x=[263, 1703], edge_index=[2, 692], y=[263], num_nodes=263, train_id=[157], val_id=[53], test_id=[53], train_mask=[263], val_mask=[263], test_mask=[263])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "source": [
    "print(data)\n",
    "print(len(clean_text))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data(x=[263, 1703], edge_index=[2, 692], y=[263], num_nodes=263, train_id=[157], val_id=[53], test_id=[53], train_mask=[263], val_mask=[263], test_mask=[263])\n",
      "263\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "source": [
    "data, clean_text = get_raw_text_webkb('texas', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/texas/http:^^www.cs.utexas.edu^ not found\n",
      "Data(x=[187, 1703], edge_index=[2, 578], y=[187], num_nodes=187, train_id=[112], val_id=[37], test_id=[38], train_mask=[187], val_mask=[187], test_mask=[187])\n",
      "0\n",
      "Data(x=[186, 1703], edge_index=[2, 370], y=[186], num_nodes=186, train_id=[112], val_id=[36], test_id=[38], train_mask=[186], val_mask=[186], test_mask=[186])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "source": [
    "print(data)\n",
    "print(len(clean_text))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data(x=[186, 1703], edge_index=[2, 370], y=[186], num_nodes=186, train_id=[112], val_id=[36], test_id=[38], train_mask=[186], val_mask=[186], test_mask=[186])\n",
      "186\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "source": [
    "data, clean_text = get_raw_text_webkb('cornell', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "12 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/cornell/http:^^www.cs.cornell.edu^ not found\n",
      "Data(x=[195, 1703], edge_index=[2, 569], y=[195], num_nodes=195, train_id=[117], val_id=[39], test_id=[39], train_mask=[195], val_mask=[195], test_mask=[195])\n",
      "12\n",
      "Data(x=[194, 1703], edge_index=[2, 381], y=[194], num_nodes=194, train_id=[116], val_id=[39], test_id=[39], train_mask=[194], val_mask=[194], test_mask=[194])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "source": [
    "print(data)\n",
    "print(len(clean_text))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data(x=[194, 1703], edge_index=[2, 381], y=[194], num_nodes=194, train_id=[116], val_id=[39], test_id=[39], train_mask=[194], val_mask=[194], test_mask=[194])\n",
      "194\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "source": [
    "data, clean_text = get_raw_text_webkb('wisconsin', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/wisconsin/http:^^www.cs.wisc.edu^ not found\n",
      "5 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/wisconsin/http:^^www.cs.wisc.edu^condor^next.html not found\n",
      "Data(x=[265, 1703], edge_index=[2, 938], y=[265], num_nodes=265, train_id=[159], val_id=[53], test_id=[53], train_mask=[265], val_mask=[265], test_mask=[265])\n",
      "5\n",
      "Data(x=[264, 1703], edge_index=[2, 936], y=[264], num_nodes=264, train_id=[158], val_id=[53], test_id=[53], train_mask=[264], val_mask=[264], test_mask=[264])\n",
      "3\n",
      "Data(x=[263, 1703], edge_index=[2, 692], y=[263], num_nodes=263, train_id=[157], val_id=[53], test_id=[53], train_mask=[263], val_mask=[263], test_mask=[263])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "source": [
    "print(data)\n",
    "print(len(clean_text))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data(x=[263, 1703], edge_index=[2, 692], y=[263], num_nodes=263, train_id=[157], val_id=[53], test_id=[53], train_mask=[263], val_mask=[263], test_mask=[263])\n",
      "263\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "source": [
    "data, clean_text = get_raw_text_webkb('wisconsin', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/course/wisconsin/http:^^www.cs.wisc.edu^ not found\n",
      "5 /storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw/project/wisconsin/http:^^www.cs.wisc.edu^condor^next.html not found\n",
      "Data(x=[265, 1703], edge_index=[2, 938], y=[265], num_nodes=265, train_id=[159], val_id=[53], test_id=[53], train_mask=[265], val_mask=[265], test_mask=[265])\n",
      "5\n",
      "Data(x=[264, 1703], edge_index=[2, 936], y=[264], num_nodes=264, train_id=[158], val_id=[53], test_id=[53], train_mask=[264], val_mask=[264], test_mask=[264])\n",
      "3\n",
      "Data(x=[263, 1703], edge_index=[2, 692], y=[263], num_nodes=263, train_id=[157], val_id=[53], test_id=[53], train_mask=[263], val_mask=[263], test_mask=[263])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "source": [
    "print(data)\n",
    "print(len(clean_text))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data(x=[263, 1703], edge_index=[2, 692], y=[263], num_nodes=263, train_id=[157], val_id=[53], test_id=[53], train_mask=[263], val_mask=[263], test_mask=[263])\n",
      "263\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "source": [
    "import torch\n",
    "edge_index = data.edge_index\n",
    "# 假设 edge_index 是一个大小为 (2, edge_num) 的张量\n",
    "# 假设 n 是节点的数量\n",
    "\n",
    "# 获取节点的数量 n\n",
    "n = data.nodes_num  # 你需要将这个值替换为你实际的节点数量\n",
    "\n",
    "# 检查 edge_index 中的边是否超出节点范围\n",
    "out_of_range_edges = (edge_index[0] < 0) | (edge_index[0] >= n) | (edge_index[1] < 0) | (edge_index[1] >= n)\n",
    "\n",
    "# 如果 out_of_range_edges 中存在 True 值，表示有边超出节点范围\n",
    "if out_of_range_edges.any():\n",
    "    print(\"存在超出节点范围的边。\")\n",
    "else:\n",
    "    print(\"所有边都在节点范围内。\")"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'GlobalStorage' object has no attribute 'nodes_num'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/storage/qiaoyr/TAPE/core/data_utils/load_webkb.py\u001b[0m in \u001b[0;36mline 8\n\u001b[1;32m      <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=184'>185</a>\u001b[0m edge_index \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39medge_index\n\u001b[1;32m      <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=185'>186</a>\u001b[0m \u001b[39m# 假设 edge_index 是一个大小为 (2, edge_num) 的张量\u001b[39;00m\n\u001b[1;32m      <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=186'>187</a>\u001b[0m \u001b[39m# 假设 n 是节点的数量\u001b[39;00m\n\u001b[1;32m      <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=187'>188</a>\u001b[0m \n\u001b[1;32m      <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=188'>189</a>\u001b[0m \u001b[39m# 获取节点的数量 n\u001b[39;00m\n\u001b[0;32m----> <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=189'>190</a>\u001b[0m n \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mnodes_num  \u001b[39m# 你需要将这个值替换为你实际的节点数量\u001b[39;00m\n\u001b[1;32m     <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=191'>192</a>\u001b[0m \u001b[39m# 检查 edge_index 中的边是否超出节点范围\u001b[39;00m\n\u001b[1;32m     <a href='file:///storage/qiaoyr/TAPE/core/data_utils/load_webkb.py?line=192'>193</a>\u001b[0m out_of_range_edges \u001b[39m=\u001b[39m (edge_index[\u001b[39m0\u001b[39m] \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m|\u001b[39m (edge_index[\u001b[39m0\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m n) \u001b[39m|\u001b[39m (edge_index[\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m|\u001b[39m (edge_index[\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m n)\n",
      "File \u001b[0;32m~/miniconda3/envs/LG/lib/python3.8/site-packages/torch_geometric/data/data.py:482\u001b[0m, in \u001b[0;36mData.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/torch_geometric/data/data.py?line=475'>476</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_store\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m    <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/torch_geometric/data/data.py?line=476'>477</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/torch_geometric/data/data.py?line=477'>478</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m\u001b[39m object was created by an older version of PyG. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/torch_geometric/data/data.py?line=478'>479</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf this error occurred while loading an already existing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/torch_geometric/data/data.py?line=479'>480</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdataset, remove the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mprocessed/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m directory in the dataset\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/torch_geometric/data/data.py?line=480'>481</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mroot folder and try again.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/torch_geometric/data/data.py?line=481'>482</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_store, key)\n",
      "File \u001b[0;32m~/miniconda3/envs/LG/lib/python3.8/site-packages/torch_geometric/data/storage.py:87\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/torch_geometric/data/storage.py?line=84'>85</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[key]\n\u001b[1;32m     <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/torch_geometric/data/storage.py?line=85'>86</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/torch_geometric/data/storage.py?line=86'>87</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m     <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/torch_geometric/data/storage.py?line=87'>88</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///home/qiaoyr/miniconda3/envs/LG/lib/python3.8/site-packages/torch_geometric/data/storage.py?line=88'>89</a>\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GlobalStorage' object has no attribute 'nodes_num'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "source": [
    "import torch\n",
    "edge_index = data.edge_index\n",
    "# 假设 edge_index 是一个大小为 (2, edge_num) 的张量\n",
    "# 假设 n 是节点的数量\n",
    "\n",
    "# 获取节点的数量 n\n",
    "n = data.num_nodes  # 你需要将这个值替换为你实际的节点数量\n",
    "\n",
    "# 检查 edge_index 中的边是否超出节点范围\n",
    "out_of_range_edges = (edge_index[0] < 0) | (edge_index[0] >= n) | (edge_index[1] < 0) | (edge_index[1] >= n)\n",
    "\n",
    "# 如果 out_of_range_edges 中存在 True 值，表示有边超出节点范围\n",
    "if out_of_range_edges.any():\n",
    "    print(\"存在超出节点范围的边。\")\n",
    "else:\n",
    "    print(\"所有边都在节点范围内。\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "存在超出节点范围的边。\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "source": [
    "import torch\n",
    "edge_index = data.edge_index\n",
    "# 假设 edge_index 是一个大小为 (2, edge_num) 的张量\n",
    "# 假设 n 是节点的数量\n",
    "\n",
    "# 获取节点的数量 n\n",
    "n = data.num_nodes  # 你需要将这个值替换为你实际的节点数量\n",
    "\n",
    "# 检查 edge_index 中的边是否超出节点范围\n",
    "out_of_range_edges = (edge_index[0] < 0) | (edge_index[0] >= n) | (edge_index[1] < 0) | (edge_index[1] >= n)\n",
    "\n",
    "# 如果 out_of_range_edges 中存在 True 值，表示有边超出节点范围\n",
    "if out_of_range_edges.any():\n",
    "    print(out_of_range_edges)\n",
    "    print(\"存在超出节点范围的边。\")\n",
    "else:\n",
    "    print(\"所有边都在节点范围内。\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([False, False, False,  True, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True])\n",
      "存在超出节点范围的边。\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_webkb(data_name):\n",
    "    path = f'/storage/qiaoyr/TAPE/dataset/web_kb/WebKB/{data_name}'\n",
    "    webpage_features_labels = np.genfromtxt(\"{}.content\".format(path), dtype=np.dtype(str))\n",
    "    data_X = webpage_features_labels[:, 1:-1].astype(np.float32)\n",
    "    labels = webpage_features_labels[:, -1]\n",
    "    #print(labels)\n",
    "    class_map = {x: i for i, x in enumerate(['course', 'faculty', 'student','project', 'staff'])}  \n",
    "    #print(class_map)\n",
    "    data_Y = np.array([class_map[x] for x in labels])\n",
    "    data_webpage_url = webpage_features_labels[:, 0]\n",
    "    # data_webpage_id = np.arange(len(data_webpage_url))\n",
    "    data_webpage_id_map = {x: i for i, x in enumerate(data_webpage_url)}\n",
    "    edges_unordered = np.genfromtxt(\"{}.cites\".format(path), dtype=np.dtype(str))\n",
    "    '''\n",
    "    for i in range(edges_unordered.shape[0]):\n",
    "        if edges_unordered[i][0] == edges_unordered[i][1]:\n",
    "            print('self loop:',edges_unordered[i][0])\n",
    "    '''\n",
    "    edges = np.array(list(map(data_webpage_id_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    #print(edges.shape)\n",
    "    data_edges = np.array(edges[~(edges == None).max(1)], dtype=np.int32)\n",
    "    #print(data_edges.shape)\n",
    "    data_edges = np.vstack((data_edges, np.fliplr(data_edges)))\n",
    "    #print(data_edges.shape)\n",
    "\n",
    "    return data_X, data_Y, data_webpage_url, np.unique(data_edges, axis=0).transpose()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "source": [
    "'''\n",
    "X, Y, webpage_id, edges = parse_wisconsin()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(webpage_id.shape)\n",
    "print(edges.shape)\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nX, Y, webpage_id, edges = parse_wisconsin()\\nprint(X.shape)\\nprint(Y.shape)\\nprint(webpage_id.shape)\\nprint(edges.shape)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 191
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "source": [
    "def get_webkb_casestudy(data_name, SEED=0):\n",
    "    data_X, data_Y, data_webpage_url, data_edges = parse_webkb(data_name)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)  # Numpy module.\n",
    "    random.seed(SEED)  # Python random module.\n",
    "\n",
    "    # load data\n",
    "    data = Data(x=torch.tensor(data_X).float(),\n",
    "                 edge_index=torch.tensor(data_edges).long(), \n",
    "                 y=torch.tensor(data_Y).long(),\n",
    "                 num_nodes=len(data_Y))\n",
    "    # split data\n",
    "    node_id = np.arange(data.num_nodes)\n",
    "    np.random.shuffle(node_id)\n",
    "\n",
    "    data.train_id = np.sort(node_id[:int(data.num_nodes * 0.6)])\n",
    "    data.val_id = np.sort(\n",
    "        node_id[int(data.num_nodes * 0.6):int(data.num_nodes * 0.8)])\n",
    "    data.test_id = np.sort(node_id[int(data.num_nodes * 0.8):])\n",
    "\n",
    "    data.train_mask = torch.tensor(\n",
    "        [x in data.train_id for x in range(data.num_nodes)])\n",
    "    data.val_mask = torch.tensor(\n",
    "        [x in data.val_id for x in range(data.num_nodes)])\n",
    "    data.test_mask = torch.tensor(\n",
    "        [x in data.test_id for x in range(data.num_nodes)])\n",
    "    \n",
    "    return data, data_webpage_url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "source": [
    "def html_process(input_string):\n",
    "    # 使用正则表达式去掉所有 HTML 标签\n",
    "    lines = input_string.split('\\n')\n",
    "    clean_text = ' '.join(lines[6:])\n",
    "\n",
    "    #non_empty_lines = [line for line in clean_text if line.strip()]\n",
    "    \n",
    "    #tag_list = ['<.*?>', r'<ahref\\s*=\\s*\".*?\"\\s*>', r'<a\\shref\\s*=\\s*\".*?\"\\s*>', r'<meta *.html>', r'<img src*>', r'<IMG SRC*\">', r'<bodyBACKGROUND*>', r'<imgsrc*>', r'<AHREF*>', '\\n']\n",
    "    tag_list = ['<.*?>', '\\n', r'<a\\s+href\\s*=\\s*\".*?\"\\s*>', r'<IMG\\s+SRC\\s*=\\s*\".*?\"\\s+ALT\\s*=\\s*\".*?\"\\s*>']\n",
    "    for tag in tag_list:\n",
    "        clean_text = re.sub(tag, '', clean_text, flags=re.IGNORECASE)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    return clean_text\n",
    "'''\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        try:\n",
    "            file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        except:\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            text.append('')\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\ndef get_raw_text_webkb(data_name, use_text=False, seed=0):\\n    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\\n    if not use_text:\\n        return data, None\\n    text = []\\n    clean_text = []\\n    category_list = ['course', 'faculty', 'student','project', 'staff']\\n    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\\n    # print(data.y.shape)\\n    # for category in category_list:\\n        # webpages = os.listdir('{}/{}'.format(path, category))\\n    for i, url in enumerate(data_webpage_url):\\n        label = data.y[i]\\n        url = url.replace('/', '^')\\n        if not url.endswith('.html'):\\n            url += '^'\\n        try:\\n            file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\\n            t = open(file_path, 'r', errors='ignore').read()\\n            text.append(t)\\n        except:\\n            print(i, file_path, 'not found') ###TODO\\n            text.append('')\\n    for t in text:\\n        clean = html_process(t)\\n        clean_text.append(clean)\\n    return data, clean_text\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 193
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "source": [
    "def delete_vacant_webpage(data, i):\n",
    "    data.y = torch.cat((data.y[:i], data.y[(i+1):]))\n",
    "    # data.edge_index = torch.cat((data.edge_index[:,:i], data.edge_index[:,(i+1):]), dim=1)\n",
    "    data.x = torch.cat((data.x[:i], data.x[(i+1):]))\n",
    "    data.train_mask = torch.cat((data.train_mask[:i], data.train_mask[(i+1):]))\n",
    "    data.val_mask = torch.cat((data.val_mask[:i], data.val_mask[(i+1):]))\n",
    "    data.test_mask = torch.cat((data.test_mask[:i], data.test_mask[(i+1):]))\n",
    "    # data.train_id = np.delete(data.train_id, i)\n",
    "    data.train_id = np.array(data.train_mask.nonzero().flatten())\n",
    "    data.val_id = np.array(data.val_mask.nonzero().flatten())\n",
    "    data.test_id = np.array(data.test_mask.nonzero().flatten())\n",
    "    data.num_nodes -= 1\n",
    "    mask = (data.edge_index[0] == i) | (data.edge_index[1] == i)\n",
    "    data.edge_index = data.edge_index[:,~mask] \n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "source": [
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        pages_to_remove = []\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "        if os.path.exists(file_path):\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        else:\n",
    "            pages_to_remove.append(i)\n",
    "            # print(i, file_path, 'not found') ###TODO\n",
    "            # text.append('')\n",
    "            \n",
    "    if data_name == 'wisconsin':\n",
    "        pages_to_remove = [3,5]\n",
    "    elif data_name == 'cornell':\n",
    "        pages_to_remove = [12]\n",
    "    elif data_name == 'texas':\n",
    "        pages_to_remove = [0]\n",
    "    elif data_name == 'washington':\n",
    "        pages_to_remove = [1, 152, 156,170,171,178,214,227]\n",
    "\n",
    "    for i in reversed(pages_to_remove):\n",
    "        data = delete_vacant_webpage(data, i)\n",
    "    edge_index = data.edge_index\n",
    "    out_of_range_edges = (edge_index[0] < 0) | (edge_index[0] >= n) | (edge_index[1] < 0) | (edge_index[1] >= n)\n",
    "    data.edge_index = data.edge_index[:,~out_of_range_edges]\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "source": [
    "data, clean_text = get_raw_text_webkb('wisconsin', use_text=True, seed=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "source": [
    "print(data)\n",
    "print(len(clean_text))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data(x=[263, 1703], edge_index=[2, 690], y=[263], num_nodes=263, train_id=[157], val_id=[53], test_id=[53], train_mask=[263], val_mask=[263], test_mask=[263])\n",
      "263\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "source": [
    "import torch\n",
    "edge_index = data.edge_index\n",
    "# 假设 edge_index 是一个大小为 (2, edge_num) 的张量\n",
    "# 假设 n 是节点的数量\n",
    "\n",
    "# 获取节点的数量 n\n",
    "n = data.num_nodes  # 你需要将这个值替换为你实际的节点数量\n",
    "\n",
    "# 检查 edge_index 中的边是否超出节点范围\n",
    "out_of_range_edges = (edge_index[0] < 0) | (edge_index[0] >= n) | (edge_index[1] < 0) | (edge_index[1] >= n)\n",
    "\n",
    "# 如果 out_of_range_edges 中存在 True 值，表示有边超出节点范围\n",
    "if out_of_range_edges.any():\n",
    "    print(out_of_range_edges)\n",
    "    print(\"存在超出节点范围的边。\")\n",
    "else:\n",
    "    print(\"所有边都在节点范围内。\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "所有边都在节点范围内。\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_webkb(data_name):\n",
    "    path = f'/storage/qiaoyr/TAPE/dataset/web_kb/WebKB/{data_name}'\n",
    "    webpage_features_labels = np.genfromtxt(\"{}.content\".format(path), dtype=np.dtype(str))\n",
    "    data_X = webpage_features_labels[:, 1:-1].astype(np.float32)\n",
    "    labels = webpage_features_labels[:, -1]\n",
    "    #print(labels)\n",
    "    class_map = {x: i for i, x in enumerate(['course', 'faculty', 'student','project', 'staff'])}  \n",
    "    #print(class_map)\n",
    "    data_Y = np.array([class_map[x] for x in labels])\n",
    "    data_webpage_url = webpage_features_labels[:, 0]\n",
    "    # data_webpage_id = np.arange(len(data_webpage_url))\n",
    "    data_webpage_id_map = {x: i for i, x in enumerate(data_webpage_url)}\n",
    "    edges_unordered = np.genfromtxt(\"{}.cites\".format(path), dtype=np.dtype(str))\n",
    "    '''\n",
    "    for i in range(edges_unordered.shape[0]):\n",
    "        if edges_unordered[i][0] == edges_unordered[i][1]:\n",
    "            print('self loop:',edges_unordered[i][0])\n",
    "    '''\n",
    "    edges = np.array(list(map(data_webpage_id_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    #print(edges.shape)\n",
    "    data_edges = np.array(edges[~(edges == None).max(1)], dtype=np.int32)\n",
    "    #print(data_edges.shape)\n",
    "    data_edges = np.vstack((data_edges, np.fliplr(data_edges)))\n",
    "    #print(data_edges.shape)\n",
    "\n",
    "    return data_X, data_Y, data_webpage_url, np.unique(data_edges, axis=0).transpose()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "source": [
    "'''\n",
    "X, Y, webpage_id, edges = parse_wisconsin()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(webpage_id.shape)\n",
    "print(edges.shape)\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nX, Y, webpage_id, edges = parse_wisconsin()\\nprint(X.shape)\\nprint(Y.shape)\\nprint(webpage_id.shape)\\nprint(edges.shape)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 200
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "source": [
    "def get_webkb_casestudy(data_name, SEED=0):\n",
    "    data_X, data_Y, data_webpage_url, data_edges = parse_webkb(data_name)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)  # Numpy module.\n",
    "    random.seed(SEED)  # Python random module.\n",
    "\n",
    "    # load data\n",
    "    data = Data(x=torch.tensor(data_X).float(),\n",
    "                 edge_index=torch.tensor(data_edges).long(), \n",
    "                 y=torch.tensor(data_Y).long(),\n",
    "                 num_nodes=len(data_Y))\n",
    "    # split data\n",
    "    node_id = np.arange(data.num_nodes)\n",
    "    np.random.shuffle(node_id)\n",
    "\n",
    "    data.train_id = np.sort(node_id[:int(data.num_nodes * 0.6)])\n",
    "    data.val_id = np.sort(\n",
    "        node_id[int(data.num_nodes * 0.6):int(data.num_nodes * 0.8)])\n",
    "    data.test_id = np.sort(node_id[int(data.num_nodes * 0.8):])\n",
    "\n",
    "    data.train_mask = torch.tensor(\n",
    "        [x in data.train_id for x in range(data.num_nodes)])\n",
    "    data.val_mask = torch.tensor(\n",
    "        [x in data.val_id for x in range(data.num_nodes)])\n",
    "    data.test_mask = torch.tensor(\n",
    "        [x in data.test_id for x in range(data.num_nodes)])\n",
    "    \n",
    "    return data, data_webpage_url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "source": [
    "def html_process(input_string):\n",
    "    # 使用正则表达式去掉所有 HTML 标签\n",
    "    lines = input_string.split('\\n')\n",
    "    clean_text = ' '.join(lines[6:])\n",
    "\n",
    "    #non_empty_lines = [line for line in clean_text if line.strip()]\n",
    "    \n",
    "    #tag_list = ['<.*?>', r'<ahref\\s*=\\s*\".*?\"\\s*>', r'<a\\shref\\s*=\\s*\".*?\"\\s*>', r'<meta *.html>', r'<img src*>', r'<IMG SRC*\">', r'<bodyBACKGROUND*>', r'<imgsrc*>', r'<AHREF*>', '\\n']\n",
    "    tag_list = ['<.*?>', '\\n', r'<a\\s+href\\s*=\\s*\".*?\"\\s*>', r'<IMG\\s+SRC\\s*=\\s*\".*?\"\\s+ALT\\s*=\\s*\".*?\"\\s*>']\n",
    "    for tag in tag_list:\n",
    "        clean_text = re.sub(tag, '', clean_text, flags=re.IGNORECASE)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    return clean_text\n",
    "'''\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        try:\n",
    "            file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        except:\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            text.append('')\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\ndef get_raw_text_webkb(data_name, use_text=False, seed=0):\\n    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\\n    if not use_text:\\n        return data, None\\n    text = []\\n    clean_text = []\\n    category_list = ['course', 'faculty', 'student','project', 'staff']\\n    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\\n    # print(data.y.shape)\\n    # for category in category_list:\\n        # webpages = os.listdir('{}/{}'.format(path, category))\\n    for i, url in enumerate(data_webpage_url):\\n        label = data.y[i]\\n        url = url.replace('/', '^')\\n        if not url.endswith('.html'):\\n            url += '^'\\n        try:\\n            file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\\n            t = open(file_path, 'r', errors='ignore').read()\\n            text.append(t)\\n        except:\\n            print(i, file_path, 'not found') ###TODO\\n            text.append('')\\n    for t in text:\\n        clean = html_process(t)\\n        clean_text.append(clean)\\n    return data, clean_text\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 202
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "source": [
    "def delete_vacant_webpage(data, i):\n",
    "    data.y = torch.cat((data.y[:i], data.y[(i+1):]))\n",
    "    # data.edge_index = torch.cat((data.edge_index[:,:i], data.edge_index[:,(i+1):]), dim=1)\n",
    "    data.x = torch.cat((data.x[:i], data.x[(i+1):]))\n",
    "    data.train_mask = torch.cat((data.train_mask[:i], data.train_mask[(i+1):]))\n",
    "    data.val_mask = torch.cat((data.val_mask[:i], data.val_mask[(i+1):]))\n",
    "    data.test_mask = torch.cat((data.test_mask[:i], data.test_mask[(i+1):]))\n",
    "    # data.train_id = np.delete(data.train_id, i)\n",
    "    data.train_id = np.array(data.train_mask.nonzero().flatten())\n",
    "    data.val_id = np.array(data.val_mask.nonzero().flatten())\n",
    "    data.test_id = np.array(data.test_mask.nonzero().flatten())\n",
    "    data.num_nodes -= 1\n",
    "    #mask = (data.edge_index[0] == i) | (data.edge_index[1] == i)\n",
    "    #data.edge_index = data.edge_index[:,~mask] \n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "source": [
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        pages_to_remove = []\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "        if os.path.exists(file_path):\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        else:\n",
    "            pages_to_remove.append(i)\n",
    "            # print(i, file_path, 'not found') ###TODO\n",
    "            # text.append('')\n",
    "            \n",
    "    if data_name == 'wisconsin':\n",
    "        pages_to_remove = [3,5]\n",
    "    elif data_name == 'cornell':\n",
    "        pages_to_remove = [12]\n",
    "    elif data_name == 'texas':\n",
    "        pages_to_remove = [0]\n",
    "    elif data_name == 'washington':\n",
    "        pages_to_remove = [1, 152, 156,170,171,178,214,227]\n",
    "\n",
    "    for i in reversed(pages_to_remove):\n",
    "        data = delete_vacant_webpage(data, i)\n",
    "    edge_index = data.edge_index\n",
    "    out_of_range_edges = (edge_index[0] < 0) | (edge_index[0] >= data.num_nodes) | (edge_index[1] < 0) | (edge_index[1] >= data.num_nodes)\n",
    "    print(out_of_range_edges.sum())\n",
    "    print(data.edge_index.shape)\n",
    "    data.edge_index = data.edge_index[:,~out_of_range_edges]\n",
    "    print(data.edge_index.shape)\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "source": [
    "data, clean_text = get_raw_text_webkb('wisconsin', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(4)\n",
      "torch.Size([2, 938])\n",
      "torch.Size([2, 934])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_webkb(data_name):\n",
    "    path = f'/storage/qiaoyr/TAPE/dataset/web_kb/WebKB/{data_name}'\n",
    "    webpage_features_labels = np.genfromtxt(\"{}.content\".format(path), dtype=np.dtype(str))\n",
    "    data_X = webpage_features_labels[:, 1:-1].astype(np.float32)\n",
    "    labels = webpage_features_labels[:, -1]\n",
    "    #print(labels)\n",
    "    class_map = {x: i for i, x in enumerate(['course', 'faculty', 'student','project', 'staff'])}  \n",
    "    #print(class_map)\n",
    "    data_Y = np.array([class_map[x] for x in labels])\n",
    "    data_webpage_url = webpage_features_labels[:, 0]\n",
    "    # data_webpage_id = np.arange(len(data_webpage_url))\n",
    "    data_webpage_id_map = {x: i for i, x in enumerate(data_webpage_url)}\n",
    "    edges_unordered = np.genfromtxt(\"{}.cites\".format(path), dtype=np.dtype(str))\n",
    "    '''\n",
    "    for i in range(edges_unordered.shape[0]):\n",
    "        if edges_unordered[i][0] == edges_unordered[i][1]:\n",
    "            print('self loop:',edges_unordered[i][0])\n",
    "    '''\n",
    "    edges = np.array(list(map(data_webpage_id_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    #print(edges.shape)\n",
    "    data_edges = np.array(edges[~(edges == None).max(1)], dtype=np.int32)\n",
    "    #print(data_edges.shape)\n",
    "    data_edges = np.vstack((data_edges, np.fliplr(data_edges)))\n",
    "    #print(data_edges.shape)\n",
    "\n",
    "    return data_X, data_Y, data_webpage_url, np.unique(data_edges, axis=0).transpose()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "source": [
    "'''\n",
    "X, Y, webpage_id, edges = parse_wisconsin()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(webpage_id.shape)\n",
    "print(edges.shape)\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nX, Y, webpage_id, edges = parse_wisconsin()\\nprint(X.shape)\\nprint(Y.shape)\\nprint(webpage_id.shape)\\nprint(edges.shape)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 207
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "source": [
    "def get_webkb_casestudy(data_name, SEED=0):\n",
    "    data_X, data_Y, data_webpage_url, data_edges = parse_webkb(data_name)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)  # Numpy module.\n",
    "    random.seed(SEED)  # Python random module.\n",
    "\n",
    "    # load data\n",
    "    data = Data(x=torch.tensor(data_X).float(),\n",
    "                 edge_index=torch.tensor(data_edges).long(), \n",
    "                 y=torch.tensor(data_Y).long(),\n",
    "                 num_nodes=len(data_Y))\n",
    "    # split data\n",
    "    node_id = np.arange(data.num_nodes)\n",
    "    np.random.shuffle(node_id)\n",
    "\n",
    "    data.train_id = np.sort(node_id[:int(data.num_nodes * 0.6)])\n",
    "    data.val_id = np.sort(\n",
    "        node_id[int(data.num_nodes * 0.6):int(data.num_nodes * 0.8)])\n",
    "    data.test_id = np.sort(node_id[int(data.num_nodes * 0.8):])\n",
    "\n",
    "    data.train_mask = torch.tensor(\n",
    "        [x in data.train_id for x in range(data.num_nodes)])\n",
    "    data.val_mask = torch.tensor(\n",
    "        [x in data.val_id for x in range(data.num_nodes)])\n",
    "    data.test_mask = torch.tensor(\n",
    "        [x in data.test_id for x in range(data.num_nodes)])\n",
    "    \n",
    "    return data, data_webpage_url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "source": [
    "def html_process(input_string):\n",
    "    # 使用正则表达式去掉所有 HTML 标签\n",
    "    lines = input_string.split('\\n')\n",
    "    clean_text = ' '.join(lines[6:])\n",
    "\n",
    "    #non_empty_lines = [line for line in clean_text if line.strip()]\n",
    "    \n",
    "    #tag_list = ['<.*?>', r'<ahref\\s*=\\s*\".*?\"\\s*>', r'<a\\shref\\s*=\\s*\".*?\"\\s*>', r'<meta *.html>', r'<img src*>', r'<IMG SRC*\">', r'<bodyBACKGROUND*>', r'<imgsrc*>', r'<AHREF*>', '\\n']\n",
    "    tag_list = ['<.*?>', '\\n', r'<a\\s+href\\s*=\\s*\".*?\"\\s*>', r'<IMG\\s+SRC\\s*=\\s*\".*?\"\\s+ALT\\s*=\\s*\".*?\"\\s*>']\n",
    "    for tag in tag_list:\n",
    "        clean_text = re.sub(tag, '', clean_text, flags=re.IGNORECASE)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    return clean_text\n",
    "'''\n",
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        try:\n",
    "            file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        except:\n",
    "            print(i, file_path, 'not found') ###TODO\n",
    "            text.append('')\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\ndef get_raw_text_webkb(data_name, use_text=False, seed=0):\\n    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\\n    if not use_text:\\n        return data, None\\n    text = []\\n    clean_text = []\\n    category_list = ['course', 'faculty', 'student','project', 'staff']\\n    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\\n    # print(data.y.shape)\\n    # for category in category_list:\\n        # webpages = os.listdir('{}/{}'.format(path, category))\\n    for i, url in enumerate(data_webpage_url):\\n        label = data.y[i]\\n        url = url.replace('/', '^')\\n        if not url.endswith('.html'):\\n            url += '^'\\n        try:\\n            file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\\n            t = open(file_path, 'r', errors='ignore').read()\\n            text.append(t)\\n        except:\\n            print(i, file_path, 'not found') ###TODO\\n            text.append('')\\n    for t in text:\\n        clean = html_process(t)\\n        clean_text.append(clean)\\n    return data, clean_text\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 209
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "source": [
    "def delete_vacant_webpage(data, i):\n",
    "    data.y = torch.cat((data.y[:i], data.y[(i+1):]))\n",
    "    # data.edge_index = torch.cat((data.edge_index[:,:i], data.edge_index[:,(i+1):]), dim=1)\n",
    "    data.x = torch.cat((data.x[:i], data.x[(i+1):]))\n",
    "    data.train_mask = torch.cat((data.train_mask[:i], data.train_mask[(i+1):]))\n",
    "    data.val_mask = torch.cat((data.val_mask[:i], data.val_mask[(i+1):]))\n",
    "    data.test_mask = torch.cat((data.test_mask[:i], data.test_mask[(i+1):]))\n",
    "    # data.train_id = np.delete(data.train_id, i)\n",
    "    data.train_id = np.array(data.train_mask.nonzero().flatten())\n",
    "    data.val_id = np.array(data.val_mask.nonzero().flatten())\n",
    "    data.test_id = np.array(data.test_mask.nonzero().flatten())\n",
    "    data.num_nodes -= 1\n",
    "    mask = (data.edge_index[0] == i) | (data.edge_index[1] == i)\n",
    "    data.edge_index = data.edge_index[:,~mask] \n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "source": [
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        pages_to_remove = []\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "        if os.path.exists(file_path):\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        else:\n",
    "            pages_to_remove.append(i)\n",
    "            # print(i, file_path, 'not found') ###TODO\n",
    "            # text.append('')\n",
    "            \n",
    "    if data_name == 'wisconsin':\n",
    "        pages_to_remove = [3,5]\n",
    "    elif data_name == 'cornell':\n",
    "        pages_to_remove = [12]\n",
    "    elif data_name == 'texas':\n",
    "        pages_to_remove = [0]\n",
    "    elif data_name == 'washington':\n",
    "        pages_to_remove = [1, 152, 156,170,171,178,214,227]\n",
    "\n",
    "    for i in reversed(pages_to_remove):\n",
    "        data = delete_vacant_webpage(data, i)\n",
    "    edge_index = data.edge_index\n",
    "    out_of_range_edges = (edge_index[0] < 0) | (edge_index[0] >= data.num_nodes) | (edge_index[1] < 0) | (edge_index[1] >= data.num_nodes)\n",
    "    print(out_of_range_edges.sum())\n",
    "    print(data.edge_index.shape)\n",
    "    data.edge_index = data.edge_index[:,~out_of_range_edges]\n",
    "    print(data.edge_index.shape)\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "source": [
    "def get_raw_text_webkb(data_name, use_text=False, seed=0):\n",
    "    data, data_webpage_url = get_webkb_casestudy(data_name, seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    text = []\n",
    "    clean_text = []\n",
    "    category_list = ['course', 'faculty', 'student','project', 'staff']\n",
    "    path = '/storage/qiaoyr/TAPE/dataset/web_kb_orig/webkb_raw'\n",
    "    # print(data.y.shape)\n",
    "    # for category in category_list:\n",
    "        # webpages = os.listdir('{}/{}'.format(path, category))\n",
    "    for i, url in enumerate(data_webpage_url):\n",
    "        label = data.y[i]\n",
    "        url = url.replace('/', '^')\n",
    "        pages_to_remove = []\n",
    "        if not url.endswith('.html'):\n",
    "            url += '^'\n",
    "        file_path = '{}/{}/{}/{}'.format(path, category_list[label], data_name, url)\n",
    "        if os.path.exists(file_path):\n",
    "            t = open(file_path, 'r', errors='ignore').read()\n",
    "            text.append(t)\n",
    "        else:\n",
    "            pages_to_remove.append(i)\n",
    "            # print(i, file_path, 'not found') ###TODO\n",
    "            # text.append('')\n",
    "            \n",
    "    if data_name == 'wisconsin':\n",
    "        pages_to_remove = [3,5]\n",
    "    elif data_name == 'cornell':\n",
    "        pages_to_remove = [12]\n",
    "    elif data_name == 'texas':\n",
    "        pages_to_remove = [0]\n",
    "    elif data_name == 'washington':\n",
    "        pages_to_remove = [1, 152, 156,170,171,178,214,227]\n",
    "\n",
    "    for i in reversed(pages_to_remove):\n",
    "        data = delete_vacant_webpage(data, i)\n",
    "    edge_index = data.edge_index\n",
    "    out_of_range_edges = (edge_index[0] < 0) | (edge_index[0] >= data.num_nodes) | (edge_index[1] < 0) | (edge_index[1] >= data.num_nodes)\n",
    "    print(out_of_range_edges.sum())\n",
    "    print(data.edge_index.shape)\n",
    "    data.edge_index = data.edge_index[:,~out_of_range_edges]\n",
    "    print(data.edge_index.shape)\n",
    "    for t in text:\n",
    "        clean = html_process(t)\n",
    "        clean_text.append(clean)\n",
    "    return data, clean_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "source": [
    "data, clean_text = get_raw_text_webkb('wisconsin', use_text=True, seed=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(2)\n",
      "torch.Size([2, 692])\n",
      "torch.Size([2, 690])\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 }
}